{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreSumm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source**:\n",
    "\n",
    "Code: https://github.com/nlpyang/PreSumm/\n",
    "\n",
    "\n",
    "Paper: https://arxiv.org/abs/1908.08345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**: \n",
    "\n",
    "Torch 1.1.0 (download instructions from https://pytorch.org/get-started/previous-versions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stanford CoreNLP**\n",
    "\n",
    "We will need Stanford CoreNLP to tokenize the data. Download it [here](https://stanfordnlp.github.io/CoreNLP/) and unzip it. Then add the following command to your bash_profile:\n",
    "```\n",
    "export CLASSPATH=/path/to/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar\n",
    "```\n",
    "replacing `/path/to/` with the path to where you saved the `stanford-corenlp-full-2017-06-09` directory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from others.tokenization import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import subprocess\n",
    "import torch\n",
    "import lxml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening data for north_carolina.xz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gufra\\onedrive\\documents\\academics\\advancedtopicsindatascience\\final_project\\presumm\\venv\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "base_path = \"./data/xml\"\n",
    "state='north_carolina.xz'\n",
    "f = lzma.open(os.path.join(base_path,state),\"rb\")\n",
    "state_data = f.readlines()\n",
    "f.close()\n",
    "data_json = [json.loads(line) for line in state_data]\n",
    "print(f'Flattening data for {state}')\n",
    "data = json_normalize(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['decision_date_p'] = pd.to_datetime(data.decision_date,errors='coerce')\n",
    "data['decision_year'] = data.decision_date_p.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2008 = data[data.decision_year>=2008]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(raw_path,save_path):\n",
    "    stories_dir = os.path.abspath(raw_path)\n",
    "    tokenized_stories_dir = os.path.abspath(save_path)\n",
    "\n",
    "    print(\"Preparing to tokenize %s to %s...\" % (stories_dir, tokenized_stories_dir))\n",
    "    stories = os.listdir(stories_dir)\n",
    "    # make IO list file\n",
    "    print(\"Making list of files to tokenize...\")\n",
    "    with open(\"mapping_for_corenlp.txt\", \"w\") as f:\n",
    "        for s in stories:\n",
    "            f.write(\"%s\\n\" % (os.path.join(stories_dir, s)))\n",
    "    command = ['java', 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize,ssplit',\n",
    "               '-ssplit.newlineIsSentenceBreak', 'always', '-filelist', 'mapping_for_corenlp.txt', '-outputFormat',\n",
    "               'json', '-outputDirectory', tokenized_stories_dir]\n",
    "    print(\"Tokenizing %i files in %s and saving in %s...\" % (len(stories), stories_dir, tokenized_stories_dir))\n",
    "    subprocess.call(command)\n",
    "    print(\"Stanford CoreNLP Tokenizer has finished.\")\n",
    "    os.remove(\"mapping_for_corenlp.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = data_2008.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data_2008.iterrows():\n",
    "    try:\n",
    "        caseid = row[1].id\n",
    "        markup = row[1]['casebody.data']\n",
    "        soup = BeautifulSoup(markup, \"xml\")\n",
    "        opinion = soup.find_all('opinion')[0]\n",
    "        opinion_text = opinion.getText()\n",
    "        opinion_text = opinion_text.encode(\"ascii\", \"ignore\").strip().decode(\"ascii\")\n",
    "        headnotes = (' '.join([headnotes.getText() for headnotes in soup.find_all('headnotes')])).replace('\\n', ' ')\n",
    "        headnotes = headnotes.encode(\"ascii\", \"ignore\").strip().decode(\"ascii\")\n",
    "\n",
    "        if (len(headnotes) > 150 and len(opinion_text)>len(headnotes)):\n",
    "            with open(f'presumm_data/parsed_text/opinions/{caseid}.txt','w') as f:\n",
    "                f.write(opinion_text)\n",
    "\n",
    "            with open(f'presumm_data/parsed_text/headnotes/{caseid}.txt','w') as f:\n",
    "                f.write(headnotes)\n",
    "    except:\n",
    "        print(f'Case ID {caseid} parsing failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to tokenize C:\\Users\\gufra\\OneDrive\\Documents\\Academics\\AdvancedTopicsInDataScience\\final_project\\presumm_data\\parsed_text\\opinions to C:\\Users\\gufra\\OneDrive\\Documents\\Academics\\AdvancedTopicsInDataScience\\final_project\\presumm_data\\tokenized_text\\opinions...\n",
      "Making list of files to tokenize...\n",
      "Tokenizing 3693 files in C:\\Users\\gufra\\OneDrive\\Documents\\Academics\\AdvancedTopicsInDataScience\\final_project\\presumm_data\\parsed_text\\opinions and saving in C:\\Users\\gufra\\OneDrive\\Documents\\Academics\\AdvancedTopicsInDataScience\\final_project\\presumm_data\\tokenized_text\\opinions...\n",
      "Stanford CoreNLP Tokenizer has finished.\n"
     ]
    }
   ],
   "source": [
    "parsed_opinions_path = 'presumm_data/parsed_text/opinions'\n",
    "tokenized_opinions_path = 'presumm_data/tokenized_text/opinions'\n",
    "tokenize(parsed_opinions_path,tokenized_opinions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to tokenize C:\\Users\\gufra\\OneDrive\\Documents\\Academics\\AdvancedTopicsInDataScience\\final_project\\presumm_data\\parsed_text\\headnotes to C:\\Users\\gufra\\OneDrive\\Documents\\Academics\\AdvancedTopicsInDataScience\\final_project\\presumm_data\\tokenized_text\\headnotes...\n",
      "Making list of files to tokenize...\n",
      "Tokenizing 3693 files in C:\\Users\\gufra\\OneDrive\\Documents\\Academics\\AdvancedTopicsInDataScience\\final_project\\presumm_data\\parsed_text\\headnotes and saving in C:\\Users\\gufra\\OneDrive\\Documents\\Academics\\AdvancedTopicsInDataScience\\final_project\\presumm_data\\tokenized_text\\headnotes...\n",
      "Stanford CoreNLP Tokenizer has finished.\n"
     ]
    }
   ],
   "source": [
    "parsed_headnotes_path = 'presumm_data/parsed_text/headnotes'\n",
    "tokenized_headnotes_path = 'presumm_data/tokenized_text/headnotes'\n",
    "tokenize(parsed_headnotes_path,tokenized_headnotes_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REMAP = {\"-lrb-\": \"(\", \"-rrb-\": \")\", \"-lcb-\": \"{\", \"-rcb-\": \"}\",\n",
    "         \"-lsb-\": \"[\", \"-rsb-\": \"]\", \"``\": '\"', \"''\": '\"'}\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    return re.sub(\n",
    "        r\"-lrb-|-rrb-|-lcb-|-rcb-|-lsb-|-rsb-|``|''\",\n",
    "        lambda m: REMAP.get(m.group()), x)\n",
    "\n",
    "def load_json(case_id):\n",
    "    source = []\n",
    "    tgt = []\n",
    "    source_path = os.path.join('presumm_data/tokenized_text/opinions',f'{case_id}.txt.json')\n",
    "    target_path = os.path.join('presumm_data/tokenized_text/headnotes',f'{case_id}.txt.json')\n",
    "    for sent in json.load(open(source_path,encoding='utf-8'))['sentences']:\n",
    "        tokens = [t['word'].encode(\"ascii\", \"ignore\").strip().decode(\"utf-8\") for t in sent['tokens']]\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        source.append(tokens)\n",
    "    for sent in json.load(open(target_path,encoding='utf-8'))['sentences']:\n",
    "        tokens = [t['word'].encode(\"ascii\", \"ignore\").strip().decode(\"utf-8\") for t in sent['tokens']]\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        tgt.append(tokens)\n",
    "\n",
    "\n",
    "    source = [clean(' '.join(sent)).split() for sent in source]\n",
    "    tgt = [clean(' '.join(sent)).split() for sent in tgt]\n",
    "    return source, tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _get_ngrams(n, text):\n",
    "    \"\"\"Calcualtes n-grams.\n",
    "\n",
    "    Args:\n",
    "      n: which n-grams to calculate\n",
    "      text: An array of tokens\n",
    "\n",
    "    Returns:\n",
    "      A set of n-grams\n",
    "    \"\"\"\n",
    "    ngram_set = set()\n",
    "    text_length = len(text)\n",
    "    max_index_ngram_start = text_length - n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set.add(tuple(text[i:i + n]))\n",
    "    return ngram_set\n",
    "\n",
    "\n",
    "def _get_word_ngrams(n, sentences):\n",
    "    \"\"\"Calculates word n-grams for multiple sentences.\n",
    "    \"\"\"\n",
    "    assert len(sentences) > 0\n",
    "    assert n > 0\n",
    "\n",
    "    # words = _split_into_words(sentences)\n",
    "\n",
    "    words = sum(sentences, [])\n",
    "    # words = [w for w in words if w not in stopwords]\n",
    "    return _get_ngrams(n, words)\n",
    "\n",
    "\n",
    "def cal_rouge(evaluated_ngrams, reference_ngrams):\n",
    "    reference_count = len(reference_ngrams)\n",
    "    evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    if evaluated_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlapping_count / evaluated_count\n",
    "\n",
    "    if reference_count == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlapping_count / reference_count\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "    return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
    "\n",
    "\n",
    "def greedy_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "   \n",
    "    max_rouge = 0.0\n",
    "    abstract = sum(abstract_sent_list, [])\n",
    "    #abstract = abstract_sent_list\n",
    "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
    "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    #print(evaluated_1grams)\n",
    "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
    "\n",
    "    selected = []\n",
    "\n",
    "    for s in range(summary_size):\n",
    "        cur_max_rouge = max_rouge\n",
    "        cur_id = -1\n",
    "        \n",
    "        for i in range(len(sents)):\n",
    "            if (i in selected):\n",
    "                continue\n",
    "                \n",
    "            c = selected + [i]\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "            rouge_score = rouge_1 + rouge_2           \n",
    "            if rouge_score > cur_max_rouge:\n",
    "                cur_max_rouge = rouge_score\n",
    "                cur_id = i\n",
    "        if (cur_id == -1):\n",
    "            return sorted(selected)\n",
    "        selected.append(cur_id)\n",
    "        max_rouge = cur_max_rouge\n",
    "    \n",
    "    \n",
    "    return sorted(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_src_nsents =10000\n",
    "class BertData():\n",
    "    def __init__(self, min_src_ntokens_per_sent=5,\n",
    "                max_src_ntokens_per_sent=200,\n",
    "                max_src_nsents=max_src_nsents,\n",
    "                min_src_nsents=1,\n",
    "                max_tgt_ntokens=500,\n",
    "                min_tgt_ntokens=5):\n",
    "        self.min_src_ntokens_per_sent = min_src_ntokens_per_sent\n",
    "        self.max_src_ntokens_per_sent = max_src_ntokens_per_sent\n",
    "        self.max_src_nsents = max_src_nsents\n",
    "        self.min_src_nsents = min_src_nsents\n",
    "        self.max_tgt_ntokens = max_tgt_ntokens\n",
    "        self.min_tgt_ntokens = min_tgt_ntokens\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.tgt_bos = '[unused0]'\n",
    "        self.tgt_eos = '[unused1]'\n",
    "        self.tgt_sent_split = '[unused2]'\n",
    "        self.sep_vid = self.tokenizer.vocab[self.sep_token]\n",
    "        self.cls_vid = self.tokenizer.vocab[self.cls_token]\n",
    "        self.pad_vid = self.tokenizer.vocab[self.pad_token]\n",
    "\n",
    "    def preprocess(self, src, tgt, sent_labels, use_bert_basic_tokenizer=False, is_test=False):\n",
    "\n",
    "        if ((not is_test) and len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [' '.join(s) for s in src]\n",
    "\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > self.min_src_ntokens_per_sent)]\n",
    "\n",
    "        _sent_labels = [0] * len(src)\n",
    "        for l in sent_labels:\n",
    "            _sent_labels[l] = 1\n",
    "\n",
    "        src = [src[i][:self.max_src_ntokens_per_sent] for i in idxs]\n",
    "        sent_labels = [_sent_labels[i] for i in idxs]\n",
    "        src = src[:self.max_src_nsents]\n",
    "        sent_labels = sent_labels[:self.max_src_nsents]\n",
    "\n",
    "        if ((not is_test) and len(src) < self.min_src_nsents):\n",
    "            return None\n",
    "\n",
    "        src_txt = [' '.join(sent) for sent in src]\n",
    "        text = ' {} {} '.format(self.sep_token, self.cls_token).join(src_txt)\n",
    "\n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        src_subtokens = [self.cls_token] + src_subtokens + [self.sep_token]\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        sent_labels = sent_labels[:len(cls_ids)]\n",
    "\n",
    "        tgt_subtokens_str = '[unused0] ' + ' [unused2] '.join(\n",
    "            [' '.join(self.tokenizer.tokenize(' '.join(tt), use_bert_basic_tokenizer=use_bert_basic_tokenizer)) for tt in tgt]) + ' [unused1]'\n",
    "        tgt_subtoken = tgt_subtokens_str.split()[:self.max_tgt_ntokens]\n",
    "        if ((not is_test) and len(tgt_subtoken) < self.min_tgt_ntokens):\n",
    "            return None\n",
    "\n",
    "        tgt_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(tgt_subtoken)\n",
    "\n",
    "        tgt_txt = '<q>'.join([' '.join(tt) for tt in tgt])\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "\n",
    "        return src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3693"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_files = os.listdir('./presumm_data/tokenized_text/opinions')\n",
    "case_ids = [case_file.replace(\".txt.json\",\"\") for case_file in case_files]\n",
    "parsed_files = [case_id.replace(\".json\",\"\") for case_id in os.listdir('./presumm_data/json_data')]\n",
    "#case_ids = list(set(case_ids).difference(parsed_files))\n",
    "len(case_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_data(case_id):\n",
    "    source, tgt = load_json(case_id)\n",
    "    sent_labels = greedy_selection(source[:max_src_nsents], tgt, 5)\n",
    "    source = [' '.join(s).lower().split() for s in source]\n",
    "    tgt = [' '.join(s).lower().split() for s in tgt]\n",
    "    bert = BertData()\n",
    "    b_data = bert.preprocess(source, tgt, sent_labels, use_bert_basic_tokenizer=True,\n",
    "                                     is_test=False)\n",
    "    if b_data is not None:\n",
    "        src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
    "        b_data_dict = {\"src\": src_subtoken_idxs, \"tgt\": tgt_subtoken_idxs,\n",
    "                               \"src_sent_labels\": sent_labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "                               'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
    "        return (case_id,b_data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mutliprocessing_funcs import generate_bert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "pool = Pool(32)\n",
    "for b_data_tp in pool.imap_unordered(generate_bert_data,case_ids):\n",
    "    if b_data_tp is not None:\n",
    "        with open(f'./presumm_data/json_data/{b_data_tp[0]}.json', 'w') as fp:\n",
    "            json.dump(b_data_tp[1], fp)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train, test and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cases = [case_id.replace(\".json\",\"\") for case_id in os.listdir('./presumm_data/json_data/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cases = len(all_cases)\n",
    "train_cases = int(np.ceil(num_cases*0.8))\n",
    "val_cases = int(np.ceil((num_cases-train_cases)/2))\n",
    "test_cases = num_cases-val_cases-train_cases\n",
    "all_index = np.arange(num_cases)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(all_index)\n",
    "train_indices =all_index[:train_cases]\n",
    "val_indices = all_index[train_cases:train_cases+val_cases]\n",
    "test_indices = all_index[train_cases+val_cases:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cases = np.array(all_cases)[train_indices]\n",
    "val_cases = np.array(all_cases)[val_indices]\n",
    "test_cases = np.array(all_cases)[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_samples(case_list):\n",
    "    appended_samples = []\n",
    "    for case_id in case_list:\n",
    "        try:\n",
    "            with open(f'./presumm_data/json_data/{case_id}.json','r') as f:\n",
    "                case_content = f.read()\n",
    "                case_content = json.loads(case_content)\n",
    "            appended_samples.append(case_content)\n",
    "        except:\n",
    "            print(f'Error reading case {case_id}')\n",
    "    return appended_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = append_samples(train_cases)\n",
    "val_dataset = append_samples(val_cases)\n",
    "test_dataset = append_samples(test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'presumm_data/train_dataset.pt')\n",
    "torch.save(val_dataset, 'presumm_data/val_dataset.pt')\n",
    "torch.save(test_dataset, 'presumm_data/test_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample_dataset = append_samples(test_cases[:10])\n",
    "len(test_sample_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_sample_dataset, 'presumm/bert_data/sample/cnndm.test.1.bert.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_cases.json','w') as f:\n",
    "    json.dump(test_cases.tolist(),f)\n",
    "\n",
    "with open('train_cases.json','w') as f:\n",
    "    json.dump(train_cases.tolist(),f)\n",
    "\n",
    "with open('val_cases.json','w') as f:\n",
    "    json.dump(val_cases.tolist(),f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep for Matchsum data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Match Sum Data\n",
    "with open('test_cases.json','r') as f:\n",
    "    test_cases = json.load(f)\n",
    "\n",
    "text_summary=[]\n",
    "sent_id = []\n",
    "\n",
    "for case_id in test_cases:\n",
    "    \n",
    "    source, tgt = load_json(case_id)\n",
    "    sent_labels = greedy_selection(source[:max_src_nsents], tgt, 5)\n",
    "    source = [' '.join(s).lower() for s in source]\n",
    "    tgt = [' '.join(s).lower() for s in tgt]\n",
    "    #text_summary.append({'text':source, 'summary':tgt})\n",
    "    #sent_id.append({'sent_id':sent_labels})\n",
    "\n",
    "    with open('sentence_id.json','a+') as f:\n",
    "        json.dump({'sent_id':sent_labels},f)\n",
    "        f.write('\\n')\n",
    "    \n",
    "    with open('match_summ_sample.json','a+') as f:\n",
    "        json.dump({'text':source, 'summary':tgt},f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreSumm\n",
    "\n",
    "**Source**:\n",
    "\n",
    "Code: https://github.com/nlpyang/PreSumm/\n",
    "\n",
    "\n",
    "Paper: https://arxiv.org/abs/1908.08345\n",
    "\n",
    "#### Pre-requisities\n",
    "\n",
    "**Libraries**: \n",
    "\n",
    "Torch 1.1.0 (download instructions from https://pytorch.org/get-started/previous-versions/)\n",
    "\n",
    "**Stanford CoreNLP**\n",
    "\n",
    "We will need Stanford CoreNLP to tokenize the data. Download it [here](https://stanfordnlp.github.io/CoreNLP/) and unzip it. Then add the following command to your bash_profile:\n",
    "```\n",
    "export CLASSPATH=/path/to/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar\n",
    "```\n",
    "replacing `/path/to/` with the path to where you saved the `stanford-corenlp-full-2017-06-09` directory. \n",
    "\n",
    "\n",
    "#### Code\n",
    "\n",
    "from others.tokenization import BertTokenizer\n",
    "\n",
    "import lzma\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import subprocess\n",
    "import torch\n",
    "import lxml\n",
    "import numpy as np\n",
    "\n",
    "# Read data\n",
    "\n",
    "base_path = \"./data/xml\"\n",
    "state='north_carolina.xz'\n",
    "f = lzma.open(os.path.join(base_path,state),\"rb\")\n",
    "state_data = f.readlines()\n",
    "f.close()\n",
    "data_json = [json.loads(line) for line in state_data]\n",
    "print(f'Flattening data for {state}')\n",
    "data = json_normalize(data_json)\n",
    "\n",
    "data['decision_date_p'] = pd.to_datetime(data.decision_date,errors='coerce')\n",
    "data['decision_year'] = data.decision_date_p.dt.year\n",
    "\n",
    "data_2008 = data[data.decision_year>=2008]\n",
    "\n",
    "# Tokenize Data\n",
    "\n",
    "def tokenize(raw_path,save_path):\n",
    "    stories_dir = os.path.abspath(raw_path)\n",
    "    tokenized_stories_dir = os.path.abspath(save_path)\n",
    "\n",
    "    print(\"Preparing to tokenize %s to %s...\" % (stories_dir, tokenized_stories_dir))\n",
    "    stories = os.listdir(stories_dir)\n",
    "    # make IO list file\n",
    "    print(\"Making list of files to tokenize...\")\n",
    "    with open(\"mapping_for_corenlp.txt\", \"w\") as f:\n",
    "        for s in stories:\n",
    "            f.write(\"%s\\n\" % (os.path.join(stories_dir, s)))\n",
    "    command = ['java', 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize,ssplit',\n",
    "               '-ssplit.newlineIsSentenceBreak', 'always', '-filelist', 'mapping_for_corenlp.txt', '-outputFormat',\n",
    "               'json', '-outputDirectory', tokenized_stories_dir]\n",
    "    print(\"Tokenizing %i files in %s and saving in %s...\" % (len(stories), stories_dir, tokenized_stories_dir))\n",
    "    subprocess.call(command)\n",
    "    print(\"Stanford CoreNLP Tokenizer has finished.\")\n",
    "    os.remove(\"mapping_for_corenlp.txt\")\n",
    "\n",
    "\n",
    "sample_data = data_2008.iloc[:100]\n",
    "\n",
    "for row in data_2008.iterrows():\n",
    "    try:\n",
    "        caseid = row[1].id\n",
    "        markup = row[1]['casebody.data']\n",
    "        soup = BeautifulSoup(markup, \"xml\")\n",
    "        opinion = soup.find_all('opinion')[0]\n",
    "        opinion_text = opinion.getText()\n",
    "        opinion_text = opinion_text.encode(\"ascii\", \"ignore\").strip().decode(\"ascii\")\n",
    "        headnotes = (' '.join([headnotes.getText() for headnotes in soup.find_all('headnotes')])).replace('\\n', ' ')\n",
    "        headnotes = headnotes.encode(\"ascii\", \"ignore\").strip().decode(\"ascii\")\n",
    "\n",
    "        if (len(headnotes) > 150 and len(opinion_text)>len(headnotes)):\n",
    "            with open(f'presumm_data/parsed_text/opinions/{caseid}.txt','w') as f:\n",
    "                f.write(opinion_text)\n",
    "\n",
    "            with open(f'presumm_data/parsed_text/headnotes/{caseid}.txt','w') as f:\n",
    "                f.write(headnotes)\n",
    "    except:\n",
    "        print(f'Case ID {caseid} parsing failed')\n",
    "\n",
    "parsed_opinions_path = 'presumm_data/parsed_text/opinions'\n",
    "tokenized_opinions_path = 'presumm_data/tokenized_text/opinions'\n",
    "tokenize(parsed_opinions_path,tokenized_opinions_path)\n",
    "\n",
    "parsed_headnotes_path = 'presumm_data/parsed_text/headnotes'\n",
    "tokenized_headnotes_path = 'presumm_data/tokenized_text/headnotes'\n",
    "tokenize(parsed_headnotes_path,tokenized_headnotes_path)\n",
    "\n",
    "# Converting to JSON\n",
    "\n",
    "\n",
    "REMAP = {\"-lrb-\": \"(\", \"-rrb-\": \")\", \"-lcb-\": \"{\", \"-rcb-\": \"}\",\n",
    "         \"-lsb-\": \"[\", \"-rsb-\": \"]\", \"``\": '\"', \"''\": '\"'}\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    return re.sub(\n",
    "        r\"-lrb-|-rrb-|-lcb-|-rcb-|-lsb-|-rsb-|``|''\",\n",
    "        lambda m: REMAP.get(m.group()), x)\n",
    "\n",
    "def load_json(case_id):\n",
    "    source = []\n",
    "    tgt = []\n",
    "    source_path = os.path.join('presumm_data/tokenized_text/opinions',f'{case_id}.txt.json')\n",
    "    target_path = os.path.join('presumm_data/tokenized_text/headnotes',f'{case_id}.txt.json')\n",
    "    for sent in json.load(open(source_path,encoding='utf-8'))['sentences']:\n",
    "        tokens = [t['word'].encode(\"ascii\", \"ignore\").strip().decode(\"utf-8\") for t in sent['tokens']]\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        source.append(tokens)\n",
    "    for sent in json.load(open(target_path,encoding='utf-8'))['sentences']:\n",
    "        tokens = [t['word'].encode(\"ascii\", \"ignore\").strip().decode(\"utf-8\") for t in sent['tokens']]\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        tgt.append(tokens)\n",
    "\n",
    "\n",
    "    source = [clean(' '.join(sent)).split() for sent in source]\n",
    "    tgt = [clean(' '.join(sent)).split() for sent in tgt]\n",
    "    return source, tgt\n",
    "\n",
    "### Greedy Selection\n",
    "\n",
    "import re\n",
    "\n",
    "def _get_ngrams(n, text):\n",
    "    \"\"\"Calcualtes n-grams.\n",
    "\n",
    "    Args:\n",
    "      n: which n-grams to calculate\n",
    "      text: An array of tokens\n",
    "\n",
    "    Returns:\n",
    "      A set of n-grams\n",
    "    \"\"\"\n",
    "    ngram_set = set()\n",
    "    text_length = len(text)\n",
    "    max_index_ngram_start = text_length - n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set.add(tuple(text[i:i + n]))\n",
    "    return ngram_set\n",
    "\n",
    "\n",
    "def _get_word_ngrams(n, sentences):\n",
    "    \"\"\"Calculates word n-grams for multiple sentences.\n",
    "    \"\"\"\n",
    "    assert len(sentences) > 0\n",
    "    assert n > 0\n",
    "\n",
    "    # words = _split_into_words(sentences)\n",
    "\n",
    "    words = sum(sentences, [])\n",
    "    # words = [w for w in words if w not in stopwords]\n",
    "    return _get_ngrams(n, words)\n",
    "\n",
    "\n",
    "def cal_rouge(evaluated_ngrams, reference_ngrams):\n",
    "    reference_count = len(reference_ngrams)\n",
    "    evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    if evaluated_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlapping_count / evaluated_count\n",
    "\n",
    "    if reference_count == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlapping_count / reference_count\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "    return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
    "\n",
    "\n",
    "def greedy_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "   \n",
    "    max_rouge = 0.0\n",
    "    abstract = sum(abstract_sent_list, [])\n",
    "    #abstract = abstract_sent_list\n",
    "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
    "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    #print(evaluated_1grams)\n",
    "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
    "\n",
    "    selected = []\n",
    "\n",
    "    for s in range(summary_size):\n",
    "        cur_max_rouge = max_rouge\n",
    "        cur_id = -1\n",
    "        \n",
    "        for i in range(len(sents)):\n",
    "            if (i in selected):\n",
    "                continue\n",
    "                \n",
    "            c = selected + [i]\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "            rouge_score = rouge_1 + rouge_2           \n",
    "            if rouge_score > cur_max_rouge:\n",
    "                cur_max_rouge = rouge_score\n",
    "                cur_id = i\n",
    "        if (cur_id == -1):\n",
    "            return sorted(selected)\n",
    "        selected.append(cur_id)\n",
    "        max_rouge = cur_max_rouge\n",
    "    \n",
    "    \n",
    "    return sorted(selected)\n",
    "\n",
    "### Bert Data\n",
    "\n",
    "max_src_nsents =10000\n",
    "class BertData():\n",
    "    def __init__(self, min_src_ntokens_per_sent=5,\n",
    "                max_src_ntokens_per_sent=200,\n",
    "                max_src_nsents=max_src_nsents,\n",
    "                min_src_nsents=1,\n",
    "                max_tgt_ntokens=500,\n",
    "                min_tgt_ntokens=5):\n",
    "        self.min_src_ntokens_per_sent = min_src_ntokens_per_sent\n",
    "        self.max_src_ntokens_per_sent = max_src_ntokens_per_sent\n",
    "        self.max_src_nsents = max_src_nsents\n",
    "        self.min_src_nsents = min_src_nsents\n",
    "        self.max_tgt_ntokens = max_tgt_ntokens\n",
    "        self.min_tgt_ntokens = min_tgt_ntokens\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.tgt_bos = '[unused0]'\n",
    "        self.tgt_eos = '[unused1]'\n",
    "        self.tgt_sent_split = '[unused2]'\n",
    "        self.sep_vid = self.tokenizer.vocab[self.sep_token]\n",
    "        self.cls_vid = self.tokenizer.vocab[self.cls_token]\n",
    "        self.pad_vid = self.tokenizer.vocab[self.pad_token]\n",
    "\n",
    "    def preprocess(self, src, tgt, sent_labels, use_bert_basic_tokenizer=False, is_test=False):\n",
    "\n",
    "        if ((not is_test) and len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [' '.join(s) for s in src]\n",
    "\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > self.min_src_ntokens_per_sent)]\n",
    "\n",
    "        _sent_labels = [0] * len(src)\n",
    "        for l in sent_labels:\n",
    "            _sent_labels[l] = 1\n",
    "\n",
    "        src = [src[i][:self.max_src_ntokens_per_sent] for i in idxs]\n",
    "        sent_labels = [_sent_labels[i] for i in idxs]\n",
    "        src = src[:self.max_src_nsents]\n",
    "        sent_labels = sent_labels[:self.max_src_nsents]\n",
    "\n",
    "        if ((not is_test) and len(src) < self.min_src_nsents):\n",
    "            return None\n",
    "\n",
    "        src_txt = [' '.join(sent) for sent in src]\n",
    "        text = ' {} {} '.format(self.sep_token, self.cls_token).join(src_txt)\n",
    "\n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        src_subtokens = [self.cls_token] + src_subtokens + [self.sep_token]\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        sent_labels = sent_labels[:len(cls_ids)]\n",
    "\n",
    "        tgt_subtokens_str = '[unused0] ' + ' [unused2] '.join(\n",
    "            [' '.join(self.tokenizer.tokenize(' '.join(tt), use_bert_basic_tokenizer=use_bert_basic_tokenizer)) for tt in tgt]) + ' [unused1]'\n",
    "        tgt_subtoken = tgt_subtokens_str.split()[:self.max_tgt_ntokens]\n",
    "        if ((not is_test) and len(tgt_subtoken) < self.min_tgt_ntokens):\n",
    "            return None\n",
    "\n",
    "        tgt_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(tgt_subtoken)\n",
    "\n",
    "        tgt_txt = '<q>'.join([' '.join(tt) for tt in tgt])\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "\n",
    "        return src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt\n",
    "\n",
    "\n",
    "case_files = os.listdir('./presumm_data/tokenized_text/opinions')\n",
    "case_ids = [case_file.replace(\".txt.json\",\"\") for case_file in case_files]\n",
    "parsed_files = [case_id.replace(\".json\",\"\") for case_id in os.listdir('./presumm_data/json_data')]\n",
    "#case_ids = list(set(case_ids).difference(parsed_files))\n",
    "len(case_ids)\n",
    "\n",
    "def generate_bert_data(case_id):\n",
    "    source, tgt = load_json(case_id)\n",
    "    sent_labels = greedy_selection(source[:max_src_nsents], tgt, 5)\n",
    "    source = [' '.join(s).lower().split() for s in source]\n",
    "    tgt = [' '.join(s).lower().split() for s in tgt]\n",
    "    bert = BertData()\n",
    "    b_data = bert.preprocess(source, tgt, sent_labels, use_bert_basic_tokenizer=True,\n",
    "                                     is_test=False)\n",
    "    if b_data is not None:\n",
    "        src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
    "        b_data_dict = {\"src\": src_subtoken_idxs, \"tgt\": tgt_subtoken_idxs,\n",
    "                               \"src_sent_labels\": sent_labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "                               'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
    "        return (case_id,b_data_dict)\n",
    "\n",
    "\n",
    "from mutliprocessing_funcs import generate_bert_data\n",
    "\n",
    "from multiprocessing import Pool\n",
    "pool = Pool(32)\n",
    "for b_data_tp in pool.imap_unordered(generate_bert_data,case_ids):\n",
    "    if b_data_tp is not None:\n",
    "        with open(f'./presumm_data/json_data/{b_data_tp[0]}.json', 'w') as fp:\n",
    "            json.dump(b_data_tp[1], fp)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "### Create Train, test and validation datasets\n",
    "\n",
    "all_cases = [case_id.replace(\".json\",\"\") for case_id in os.listdir('./presumm_data/json_data/')]\n",
    "\n",
    "num_cases = len(all_cases)\n",
    "train_cases = int(np.ceil(num_cases*0.8))\n",
    "val_cases = int(np.ceil((num_cases-train_cases)/2))\n",
    "test_cases = num_cases-val_cases-train_cases\n",
    "all_index = np.arange(num_cases)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(all_index)\n",
    "train_indices =all_index[:train_cases]\n",
    "val_indices = all_index[train_cases:train_cases+val_cases]\n",
    "test_indices = all_index[train_cases+val_cases:] \n",
    "\n",
    "train_cases = np.array(all_cases)[train_indices]\n",
    "val_cases = np.array(all_cases)[val_indices]\n",
    "test_cases = np.array(all_cases)[test_indices]\n",
    "\n",
    "def append_samples(case_list):\n",
    "    appended_samples = []\n",
    "    for case_id in case_list:\n",
    "        try:\n",
    "            with open(f'./presumm_data/json_data/{case_id}.json','r') as f:\n",
    "                case_content = f.read()\n",
    "                case_content = json.loads(case_content)\n",
    "            appended_samples.append(case_content)\n",
    "        except:\n",
    "            print(f'Error reading case {case_id}')\n",
    "    return appended_samples\n",
    "\n",
    "train_dataset = append_samples(train_cases)\n",
    "val_dataset = append_samples(val_cases)\n",
    "test_dataset = append_samples(test_cases)\n",
    "\n",
    "torch.save(train_dataset, 'presumm_data/train_dataset.pt')\n",
    "torch.save(val_dataset, 'presumm_data/val_dataset.pt')\n",
    "torch.save(test_dataset, 'presumm_data/test_dataset.pt')\n",
    "\n",
    "test_sample_dataset = append_samples(test_cases[:10])\n",
    "len(test_sample_dataset)\n",
    "\n",
    "torch.save(test_sample_dataset, 'presumm/bert_data/sample/cnndm.test.1.bert.pt')\n",
    "\n",
    "with open('test_cases.json','w') as f:\n",
    "    json.dump(test_cases.tolist(),f)\n",
    "\n",
    "with open('train_cases.json','w') as f:\n",
    "    json.dump(train_cases.tolist(),f)\n",
    "\n",
    "with open('val_cases.json','w') as f:\n",
    "    json.dump(val_cases.tolist(),f)\n",
    "\n",
    "\n",
    "### Data Prep for Matchsum data\n",
    "\n",
    "# For Match Sum Data\n",
    "with open('test_cases.json','r') as f:\n",
    "    test_cases = json.load(f)\n",
    "\n",
    "text_summary=[]\n",
    "sent_id = []\n",
    "\n",
    "for case_id in test_cases:\n",
    "    \n",
    "    source, tgt = load_json(case_id)\n",
    "    sent_labels = greedy_selection(source[:max_src_nsents], tgt, 5)\n",
    "    source = [' '.join(s).lower() for s in source]\n",
    "    tgt = [' '.join(s).lower() for s in tgt]\n",
    "    #text_summary.append({'text':source, 'summary':tgt})\n",
    "    #sent_id.append({'sent_id':sent_labels})\n",
    "\n",
    "    with open('sentence_id.json','a+') as f:\n",
    "        json.dump({'sent_id':sent_labels},f)\n",
    "        f.write('\\n')\n",
    "    \n",
    "    with open('match_summ_sample.json','a+') as f:\n",
    "        json.dump({'text':source, 'summary':tgt},f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tests the baseline performance on models pretrained on CNN/DailyMail data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from train_abstractive import test_abs,train_abs_single\n",
    "from train_extractive import test_ext,train_single_ext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_params = {'accum_count':1,\n",
    "'alpha':0.6,\n",
    "'batch_size':300,\n",
    "'beam_size':5,\n",
    "'beta1':0.9,\n",
    "'beta2':0.999,\n",
    "'block_trigram':True,\n",
    "'dec_dropout':0.2,\n",
    "'dec_ff_size':2048,\n",
    "'dec_heads':8,\n",
    "'dec_hidden_size':768,\n",
    "'dec_layers':6,\n",
    "'enc_dropout':0.2,\n",
    "'enc_ff_size':512,\n",
    "'enc_hidden_size':512,\n",
    "'enc_layers':6,\n",
    "'encoder':'bert',\n",
    "'ext_dropout':0.2,\n",
    "'ext_ff_size':2048,\n",
    "'ext_heads':8,\n",
    "'ext_hidden_size':768,\n",
    "'ext_layers':2,\n",
    "'finetune_bert':True,\n",
    "'generator_shard_size':32,\n",
    "'gpu_ranks':[0],\n",
    "'label_smoothing':0.1,\n",
    "'large':False,\n",
    "'load_from_extractive':'',\n",
    "'lr':1,\n",
    "'lr_bert':0.002,\n",
    "'lr_dec':0.002,\n",
    "'max_grad_norm':0,\n",
    "'max_length':150,\n",
    "'max_pos':512,\n",
    "'max_tgt_len':140,\n",
    "'min_length':15,\n",
    "'optim':'adam',\n",
    "'param_init':0,\n",
    "'param_init_glorot':True,\n",
    "'recall_eval':False,\n",
    "'report_every':1,\n",
    "'report_rouge':True,\n",
    "'save_checkpoint_steps':15,\n",
    "'seed':666,\n",
    "'sep_optim':False,\n",
    "'share_emb':False,\n",
    "'temp_dir':'./temp',\n",
    "'test_all':False,\n",
    "'test_batch_size':200,\n",
    "'test_start_from':-1,\n",
    "'train_from':'',\n",
    "'train_steps':1000,\n",
    "'use_bert_emb':False,\n",
    "'use_interval':True,\n",
    "'visible_gpus':'-1',\n",
    "'warmup_steps':8000,\n",
    "'warmup_steps_bert':8000,\n",
    "'warmup_steps_dec':8000,\n",
    "'world_size':1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Evaluation (Pre-tained model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_args_dict = arg_params\n",
    "ext_args_dict.update({\n",
    "            'bert_data_path':'./data',\n",
    "    'log_file':'./logs/ext_baseline',\n",
    "    'model_path':'./model_files/pre_trained/ext',\n",
    "    'result_path':'./results/pre_trained/ext_baseline',\n",
    "    'test_from':'./model_files/pre_trained/ext/bertext_cnndm_transformer.pt',\n",
    "    'task':'ext',\n",
    "    'mode':'test',\n",
    "\n",
    "    'batch_size':300,\n",
    "    'ext_dropout':0.1\n",
    "})\n",
    "\n",
    "args = Namespace(**ext_args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 433/433 [00:00<00:00, 349659.92B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(accum_count=1, alpha=0.6, batch_size=300, beam_size=5, bert_data_path='./data', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.1, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='./logs/ext_baseline', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='test', model_path='./model_files/pre_trained/ext', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='./results/pre_trained/ext_baseline', save_checkpoint_steps=15, seed=666, sep_optim=False, share_emb=False, task='ext', temp_dir='./temp', test_all=False, test_batch_size=200, test_from='./model_files/pre_trained/ext/bertext_cnndm_transformer.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, visible_gpus='-1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 440473133/440473133 [00:09<00:00, 45928011.17B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts ['./data/test.pt']\n",
      "gpu_rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 17:26:44,770 [MainThread  ] [INFO ]  Writing summaries.\n",
      "2020-05-09 17:26:44,772 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./temp/tmptfgzu6h4/system and model files to ./temp/tmptfgzu6h4/model.\n",
      "2020-05-09 17:26:44,772 [MainThread  ] [INFO ]  Processing files in ./temp/rouge-tmp-2020-05-09-17-26-44/candidate/.\n",
      "2020-05-09 17:26:44,815 [MainThread  ] [INFO ]  Saved processed files to ./temp/tmptfgzu6h4/system.\n",
      "2020-05-09 17:26:44,816 [MainThread  ] [INFO ]  Processing files in ./temp/rouge-tmp-2020-05-09-17-26-44/reference/.\n",
      "2020-05-09 17:26:44,864 [MainThread  ] [INFO ]  Saved processed files to ./temp/tmptfgzu6h4/model.\n",
      "2020-05-09 17:26:44,869 [MainThread  ] [INFO ]  Written ROUGE configuration to ./temp/tmpitnfnofj/rouge_conf.xml\n",
      "2020-05-09 17:26:44,869 [MainThread  ] [INFO ]  Running ROUGE with command perl ./ROUGE-1.5.5/ROUGE-1.5.5.pl -e ./ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./temp/tmpitnfnofj/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369\n",
      "369\n",
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.24577 (95%-conf.int. 0.23134 - 0.26120)\n",
      "1 ROUGE-1 Average_P: 0.49301 (95%-conf.int. 0.47854 - 0.50676)\n",
      "1 ROUGE-1 Average_F: 0.29475 (95%-conf.int. 0.28252 - 0.30729)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.09804 (95%-conf.int. 0.08843 - 0.10867)\n",
      "1 ROUGE-2 Average_P: 0.18863 (95%-conf.int. 0.17645 - 0.20073)\n",
      "1 ROUGE-2 Average_F: 0.11580 (95%-conf.int. 0.10651 - 0.12531)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.20960 (95%-conf.int. 0.19742 - 0.22252)\n",
      "1 ROUGE-L Average_P: 0.42895 (95%-conf.int. 0.41545 - 0.44275)\n",
      "1 ROUGE-L Average_F: 0.25311 (95%-conf.int. 0.24279 - 0.26383)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_ext(args, device_id=-1, pt=args.test_from, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractive Summarization (Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_args_dict = arg_params\n",
    "\n",
    "abs_args_dict.update({\n",
    "            'bert_data_path':'./data',\n",
    "    'log_file':'./logs/abs_bertextabs',\n",
    "    'model_path':'./model_files/pre_trained/abs_bertextabs/',\n",
    "    'result_path':'./results/pre_trained/abs_bertextabs',\n",
    "    'test_from':'./model_files/pre_trained/abs_bertextabs/model_step_148000.pt',\n",
    "    'task':'abs',\n",
    "    'mode':'test',\n",
    "    \n",
    "    'batch_size':300,\n",
    "    'test_batch_size':200,\n",
    "    'max_pos':512,\n",
    "    'max_length':200,\n",
    "    'alpha': 0.95,\n",
    "    'min_length':50,\n",
    "        \n",
    "    'sep_optim':True,\n",
    "    'user_interval':True\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "args = Namespace(**abs_args_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(accum_count=1, alpha=0.95, batch_size=300, beam_size=5, bert_data_path='./data', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.1, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='./logs/abs_bertextabs', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='./model_files/pre_trained/abs_bertextabs/', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='./results/pre_trained/abs_bertextabs', save_checkpoint_steps=15, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='./temp', test_all=False, test_batch_size=200, test_from='./model_files/pre_trained/abs_bertextabs/model_step_148000.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, user_interval=True, visible_gpus='-1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n",
      "pts ['./data/test.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 6099608.21B/s]\n",
      "2020-05-09 18:29:11,656 [MainThread  ] [INFO ]  Writing summaries.\n",
      "2020-05-09 18:29:11,658 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./temp/tmpltn13h9f/system and model files to ./temp/tmpltn13h9f/model.\n",
      "2020-05-09 18:29:11,658 [MainThread  ] [INFO ]  Processing files in ./temp/rouge-tmp-2020-05-09-18-29-11/candidate/.\n",
      "2020-05-09 18:29:11,704 [MainThread  ] [INFO ]  Saved processed files to ./temp/tmpltn13h9f/system.\n",
      "2020-05-09 18:29:11,705 [MainThread  ] [INFO ]  Processing files in ./temp/rouge-tmp-2020-05-09-18-29-11/reference/.\n",
      "2020-05-09 18:29:11,757 [MainThread  ] [INFO ]  Saved processed files to ./temp/tmpltn13h9f/model.\n",
      "2020-05-09 18:29:11,762 [MainThread  ] [INFO ]  Written ROUGE configuration to ./temp/tmp1t6jzo0o/rouge_conf.xml\n",
      "2020-05-09 18:29:11,763 [MainThread  ] [INFO ]  Running ROUGE with command perl ./ROUGE-1.5.5/ROUGE-1.5.5.pl -e ./ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./temp/tmp1t6jzo0o/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369\n",
      "369\n",
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.19541 (95%-conf.int. 0.18399 - 0.20711)\n",
      "1 ROUGE-1 Average_P: 0.45892 (95%-conf.int. 0.44256 - 0.47679)\n",
      "1 ROUGE-1 Average_F: 0.24623 (95%-conf.int. 0.23525 - 0.25664)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.06171 (95%-conf.int. 0.05450 - 0.06911)\n",
      "1 ROUGE-2 Average_P: 0.14881 (95%-conf.int. 0.13608 - 0.16175)\n",
      "1 ROUGE-2 Average_F: 0.07836 (95%-conf.int. 0.07054 - 0.08633)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.16693 (95%-conf.int. 0.15691 - 0.17709)\n",
      "1 ROUGE-L Average_P: 0.40175 (95%-conf.int. 0.38643 - 0.41851)\n",
      "1 ROUGE-L Average_F: 0.21218 (95%-conf.int. 0.20277 - 0.22168)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_abs(args, device_id=-1, pt=args.test_from, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractive Summarization (Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(accum_count=1, alpha=0.95, batch_size=300, beam_size=5, bert_data_path='./data', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=512, dec_layers=6, enc_dropout=0.2, enc_ff_size=2048, enc_hidden_size=512, enc_layers=6, encoder='baseline', ext_dropout=0.1, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='./logs/abs_transformers', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='./model_files/pre_trained/abs_transformer/', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='./results/pre_trained/abs_transformer', save_checkpoint_steps=15, seed=666, sep_optim=False, share_emb=False, task='abs', temp_dir='./temp', test_all=False, test_batch_size=200, test_from='./model_files/pre_trained/abs_transformer/cnndm_baseline_best.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, user_interval=True, visible_gpus='-1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n",
      "pts ['./data/test.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:21:23,479 [MainThread  ] [INFO ]  Writing summaries.\n",
      "2020-05-09 19:21:23,480 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./temp/tmp1j8r5osy/system and model files to ./temp/tmp1j8r5osy/model.\n",
      "2020-05-09 19:21:23,481 [MainThread  ] [INFO ]  Processing files in ./temp/rouge-tmp-2020-05-09-19-21-23/candidate/.\n",
      "2020-05-09 19:21:23,527 [MainThread  ] [INFO ]  Saved processed files to ./temp/tmp1j8r5osy/system.\n",
      "2020-05-09 19:21:23,528 [MainThread  ] [INFO ]  Processing files in ./temp/rouge-tmp-2020-05-09-19-21-23/reference/.\n",
      "2020-05-09 19:21:23,582 [MainThread  ] [INFO ]  Saved processed files to ./temp/tmp1j8r5osy/model.\n",
      "2020-05-09 19:21:23,587 [MainThread  ] [INFO ]  Written ROUGE configuration to ./temp/tmpgfavjhyn/rouge_conf.xml\n",
      "2020-05-09 19:21:23,588 [MainThread  ] [INFO ]  Running ROUGE with command perl ./ROUGE-1.5.5/ROUGE-1.5.5.pl -e ./ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./temp/tmpgfavjhyn/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369\n",
      "369\n",
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.20233 (95%-conf.int. 0.19042 - 0.21442)\n",
      "1 ROUGE-1 Average_P: 0.44334 (95%-conf.int. 0.42921 - 0.45924)\n",
      "1 ROUGE-1 Average_F: 0.24971 (95%-conf.int. 0.23907 - 0.26005)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.06280 (95%-conf.int. 0.05640 - 0.07004)\n",
      "1 ROUGE-2 Average_P: 0.13759 (95%-conf.int. 0.12712 - 0.14869)\n",
      "1 ROUGE-2 Average_F: 0.07703 (95%-conf.int. 0.07082 - 0.08388)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.17118 (95%-conf.int. 0.16133 - 0.18091)\n",
      "1 ROUGE-L Average_P: 0.38531 (95%-conf.int. 0.37209 - 0.40006)\n",
      "1 ROUGE-L Average_F: 0.21326 (95%-conf.int. 0.20432 - 0.22181)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abs_args_dict = arg_params\n",
    "\n",
    "abs_args_dict.update({\n",
    "            'bert_data_path':'./data',\n",
    "    'log_file':'./logs/abs_transformers',\n",
    "    'model_path':'./model_files/pre_trained/abs_transformer/',\n",
    "    'result_path':'./results/pre_trained/abs_transformer',\n",
    "    'test_from':'./model_files/pre_trained/abs_transformer/cnndm_baseline_best.pt',\n",
    "    'task':'abs',\n",
    "    'mode':'test',\n",
    "    \n",
    "    'batch_size':300,\n",
    "    'test_batch_size':200,\n",
    "    'max_pos':512,\n",
    "    'max_length':200,\n",
    "    'min_length':50,\n",
    "        \n",
    "    'sep_optim':False,\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "args = Namespace(**abs_args_dict)\n",
    "test_abs(args, device_id=-1, pt=args.test_from, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Evaluation on Case Law Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = 30\n",
    "ext_args_train_dict = arg_params\n",
    "\n",
    "ext_args_train_dict.update({\n",
    "    \n",
    "            'bert_data_path':'./data',\n",
    "    'log_file':'./logs/train_ext',\n",
    "    'mode':'train',\n",
    "    'model_path':'./model_files/trained/ext',\n",
    "    'result_path':'../results/trained/ext',\n",
    "    'train_from':'./model_files/pre_trained/ext/bertext_cnndm_transformer.pt',\n",
    "\n",
    "    'report_every':1,\n",
    "    'save_checkpoint_steps':15,\n",
    "    'batch_size':300,\n",
    "    'train_steps':18001 + training_steps,\n",
    "    'warmup_steps':1,\n",
    "    'max_pos':512,\n",
    "\n",
    "    'task':'ext',\n",
    "    'ext_dropout':0.1,\n",
    "    'lr':0.002,\n",
    "    'accum_count':2,\n",
    "    'use_interval':True\n",
    "})\n",
    "\n",
    "\n",
    "args = Namespace(**ext_args_train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(accum_count=2, alpha=0.95, batch_size=300, beam_size=5, bert_data_path='./data', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.1, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='./logs/train_ext', lr=0.002, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='train', model_path='./model_files/trained/ext', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../results/trained/ext', save_checkpoint_steps=15, seed=666, sep_optim=False, share_emb=False, task='ext', temp_dir='./temp', test_all=False, test_batch_size=200, test_from='./model_files/pre_trained/abs_transformer/cnndm_baseline_best.pt', test_start_from=-1, train_from='./model_files/pre_trained/ext/bertext_cnndm_transformer.pt', train_steps=18031, use_bert_emb=False, use_interval=True, user_interval=True, visible_gpus='-1', warmup_steps=1, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:21:37,445 INFO] Device ID -1\n",
      "[2020-05-09 19:21:37,446 INFO] Device cpu\n",
      "[2020-05-09 19:21:37,451 INFO] Loading checkpoint from ./model_files/pre_trained/ext/bertext_cnndm_transformer.pt\n",
      "[2020-05-09 19:21:37,976 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ./temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "[2020-05-09 19:21:37,978 INFO] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2020-05-09 19:21:38,056 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ./temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'opt', 'optim'])\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:21:40,728 INFO] * number of parameters: 120512513\n",
      "[2020-05-09 19:21:40,728 INFO] Start training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_rank 0\n",
      "pts ['./data/train.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:21:42,474 INFO] Loading train dataset from ./data/train.pt, number of examples: 2955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=18001, trains_steps=18031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:21:47,806 INFO] Step 18001/18031; xent: 1.79; lr: 0.0000149;   1 docs/s;      5 sec\n",
      "[2020-05-09 19:21:52,916 INFO] Step 18002/18031; xent: 1.00; lr: 0.0000149;   1 docs/s;     10 sec\n",
      "[2020-05-09 19:21:57,968 INFO] Step 18003/18031; xent: 2.12; lr: 0.0000149;   1 docs/s;     15 sec\n",
      "[2020-05-09 19:22:03,144 INFO] Step 18004/18031; xent: 1.07; lr: 0.0000149;   1 docs/s;     21 sec\n",
      "[2020-05-09 19:22:08,255 INFO] Step 18005/18031; xent: 1.03; lr: 0.0000149;   1 docs/s;     26 sec\n",
      "[2020-05-09 19:22:13,359 INFO] Step 18006/18031; xent: 0.40; lr: 0.0000149;   1 docs/s;     31 sec\n",
      "[2020-05-09 19:22:18,460 INFO] Step 18007/18031; xent: 0.30; lr: 0.0000149;   1 docs/s;     36 sec\n",
      "[2020-05-09 19:22:23,953 INFO] Step 18008/18031; xent: 0.91; lr: 0.0000149;   1 docs/s;     41 sec\n",
      "[2020-05-09 19:22:29,269 INFO] Step 18009/18031; xent: 0.24; lr: 0.0000149;   1 docs/s;     47 sec\n",
      "[2020-05-09 19:22:34,098 INFO] Step 18010/18031; xent: 0.17; lr: 0.0000149;   1 docs/s;     52 sec\n",
      "[2020-05-09 19:22:39,210 INFO] Step 18011/18031; xent: 1.02; lr: 0.0000149;   1 docs/s;     57 sec\n",
      "[2020-05-09 19:22:44,246 INFO] Step 18012/18031; xent: 0.09; lr: 0.0000149;   1 docs/s;     62 sec\n",
      "[2020-05-09 19:22:49,329 INFO] Step 18013/18031; xent: 2.43; lr: 0.0000149;   1 docs/s;     67 sec\n",
      "[2020-05-09 19:22:54,277 INFO] Step 18014/18031; xent: 0.94; lr: 0.0000149;   1 docs/s;     72 sec\n",
      "[2020-05-09 19:22:59,156 INFO] Step 18015/18031; xent: 0.76; lr: 0.0000149;   1 docs/s;     77 sec\n",
      "[2020-05-09 19:22:59,160 INFO] Saving checkpoint ./model_files/trained/ext/model_step_18015.pt\n",
      "[2020-05-09 19:23:04,902 INFO] Step 18016/18031; xent: 2.88; lr: 0.0000149;   1 docs/s;     82 sec\n",
      "[2020-05-09 19:23:10,121 INFO] Step 18017/18031; xent: 0.07; lr: 0.0000149;   1 docs/s;     88 sec\n",
      "[2020-05-09 19:23:15,346 INFO] Step 18018/18031; xent: 2.45; lr: 0.0000149;   1 docs/s;     93 sec\n",
      "[2020-05-09 19:23:20,802 INFO] Step 18019/18031; xent: 2.14; lr: 0.0000149;   1 docs/s;     98 sec\n",
      "[2020-05-09 19:23:25,889 INFO] Step 18020/18031; xent: 0.12; lr: 0.0000149;   1 docs/s;    103 sec\n",
      "[2020-05-09 19:23:31,518 INFO] Step 18021/18031; xent: 1.93; lr: 0.0000149;   1 docs/s;    109 sec\n",
      "[2020-05-09 19:23:36,997 INFO] Step 18022/18031; xent: 2.59; lr: 0.0000149;   1 docs/s;    115 sec\n",
      "[2020-05-09 19:23:42,103 INFO] Step 18023/18031; xent: 1.66; lr: 0.0000149;   1 docs/s;    120 sec\n",
      "[2020-05-09 19:23:47,363 INFO] Step 18024/18031; xent: 0.51; lr: 0.0000149;   1 docs/s;    125 sec\n",
      "[2020-05-09 19:23:52,528 INFO] Step 18025/18031; xent: 1.16; lr: 0.0000149;   1 docs/s;    130 sec\n",
      "[2020-05-09 19:23:57,595 INFO] Step 18026/18031; xent: 0.54; lr: 0.0000149;   1 docs/s;    135 sec\n",
      "[2020-05-09 19:24:02,869 INFO] Step 18027/18031; xent: 1.42; lr: 0.0000149;   1 docs/s;    140 sec\n",
      "[2020-05-09 19:24:08,154 INFO] Step 18028/18031; xent: 2.09; lr: 0.0000149;   1 docs/s;    146 sec\n",
      "[2020-05-09 19:24:13,304 INFO] Step 18029/18031; xent: 0.80; lr: 0.0000149;   1 docs/s;    151 sec\n",
      "[2020-05-09 19:24:18,674 INFO] Step 18030/18031; xent: 1.84; lr: 0.0000149;   1 docs/s;    156 sec\n",
      "[2020-05-09 19:24:18,678 INFO] Saving checkpoint ./model_files/trained/ext/model_step_18030.pt\n",
      "[2020-05-09 19:24:24,670 INFO] Step 18031/18031; xent: 0.48; lr: 0.0000149;   1 docs/s;    162 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts ['./data/train.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:24:25,923 INFO] Loading train dataset from ./data/train.pt, number of examples: 2955\n"
     ]
    }
   ],
   "source": [
    "train_single_ext(args, device_id=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:24:26,168 INFO] Loading checkpoint from ./model_files/trained/ext/model_step_18030.pt\n",
      "[2020-05-09 19:24:26,485 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ./temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "[2020-05-09 19:24:26,486 INFO] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2020-05-09 19:24:26,561 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ./temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(accum_count=2, alpha=0.95, batch_size=300, beam_size=5, bert_data_path='./data', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.1, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='./logs/ext_trained', lr=0.002, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='./model_files/trained/ext', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='./results/trained/ext_trained', save_checkpoint_steps=15, seed=666, sep_optim=False, share_emb=False, task='ext', temp_dir='./temp', test_all=False, test_batch_size=200, test_from='./model_files/trained/ext/model_step_18030.pt', test_start_from=-1, train_from='./model_files/pre_trained/ext/bertext_cnndm_transformer.pt', train_steps=18031, use_bert_emb=False, use_interval=True, user_interval=True, visible_gpus='-1', warmup_steps=1, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:24:28,865 INFO] Loading test dataset from ./data/test.pt, number of examples: 369\n",
      "[2020-05-09 19:24:28,872 INFO] * number of parameters: 120512513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts ['./data/test.pt']\n",
      "gpu_rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:27:07,182 [MainThread  ] [INFO ]  Writing summaries.\n",
      "[2020-05-09 19:27:07,182 INFO] Writing summaries.\n",
      "2020-05-09 19:27:07,184 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./temp/tmp1osroyne/system and model files to ./temp/tmp1osroyne/model.\n",
      "[2020-05-09 19:27:07,184 INFO] Processing summaries. Saving system files to ./temp/tmp1osroyne/system and model files to ./temp/tmp1osroyne/model.\n",
      "2020-05-09 19:27:07,185 [MainThread  ] [INFO ]  Processing files in ./temp/rouge-tmp-2020-05-09-19-27-07/candidate/.\n",
      "[2020-05-09 19:27:07,185 INFO] Processing files in ./temp/rouge-tmp-2020-05-09-19-27-07/candidate/.\n",
      "2020-05-09 19:27:07,229 [MainThread  ] [INFO ]  Saved processed files to ./temp/tmp1osroyne/system.\n",
      "[2020-05-09 19:27:07,229 INFO] Saved processed files to ./temp/tmp1osroyne/system.\n",
      "2020-05-09 19:27:07,230 [MainThread  ] [INFO ]  Processing files in ./temp/rouge-tmp-2020-05-09-19-27-07/reference/.\n",
      "[2020-05-09 19:27:07,230 INFO] Processing files in ./temp/rouge-tmp-2020-05-09-19-27-07/reference/.\n",
      "2020-05-09 19:27:07,280 [MainThread  ] [INFO ]  Saved processed files to ./temp/tmp1osroyne/model.\n",
      "[2020-05-09 19:27:07,280 INFO] Saved processed files to ./temp/tmp1osroyne/model.\n",
      "2020-05-09 19:27:07,286 [MainThread  ] [INFO ]  Written ROUGE configuration to ./temp/tmpx7cq0pg9/rouge_conf.xml\n",
      "[2020-05-09 19:27:07,286 INFO] Written ROUGE configuration to ./temp/tmpx7cq0pg9/rouge_conf.xml\n",
      "2020-05-09 19:27:07,287 [MainThread  ] [INFO ]  Running ROUGE with command perl ./ROUGE-1.5.5/ROUGE-1.5.5.pl -e ./ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./temp/tmpx7cq0pg9/rouge_conf.xml\n",
      "[2020-05-09 19:27:07,287 INFO] Running ROUGE with command perl ./ROUGE-1.5.5/ROUGE-1.5.5.pl -e ./ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./temp/tmpx7cq0pg9/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369\n",
      "369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:27:21,876 INFO] Rouges at step 0 \n",
      ">> ROUGE-F(1/2/3/l): 30.20/12.20/26.04\n",
      "ROUGE-R(1/2/3/l): 25.13/10.28/21.49\n",
      "\n",
      "[2020-05-09 19:27:21,877 INFO] Validation xent: 2.43483 at step 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.25128 (95%-conf.int. 0.23658 - 0.26767)\n",
      "1 ROUGE-1 Average_P: 0.50811 (95%-conf.int. 0.49359 - 0.52156)\n",
      "1 ROUGE-1 Average_F: 0.30204 (95%-conf.int. 0.29001 - 0.31453)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.10276 (95%-conf.int. 0.09280 - 0.11371)\n",
      "1 ROUGE-2 Average_P: 0.20040 (95%-conf.int. 0.18753 - 0.21366)\n",
      "1 ROUGE-2 Average_F: 0.12198 (95%-conf.int. 0.11282 - 0.13200)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.21493 (95%-conf.int. 0.20285 - 0.22879)\n",
      "1 ROUGE-L Average_P: 0.44402 (95%-conf.int. 0.42986 - 0.45792)\n",
      "1 ROUGE-L Average_F: 0.26038 (95%-conf.int. 0.25049 - 0.27132)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ext_args_dict = arg_params\n",
    "ext_args_dict.update({\n",
    "            'bert_data_path':'./data',\n",
    "    'log_file':'./logs/ext_trained',\n",
    "    'model_path':'./model_files/trained/ext',\n",
    "    'result_path':'./results/trained/ext_trained',\n",
    "    'test_from':f'./model_files/trained/ext/model_step_{str(18001+training_steps-1)}.pt',\n",
    "    'task':'ext',\n",
    "    'mode':'test',\n",
    "\n",
    "    'batch_size':300,\n",
    "    'ext_dropout':0.1,\n",
    "    'accum_count':2\n",
    "})\n",
    "\n",
    "args = Namespace(**ext_args_dict)\n",
    "test_ext(args, device_id=-1, pt=args.test_from, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractive Summarization (Transformer Basline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = 30\n",
    "abs_args_train_dict = arg_params\n",
    "\n",
    "abs_args_train_dict.update({\n",
    "    \n",
    "            'bert_data_path':'./data',\n",
    "    'log_file':'./logs/train_abs_transformer',\n",
    "    'mode':'train',\n",
    "    'model_path':'./model_files/trained/abs_transformer',\n",
    "    'result_path':'./results/trained/abs_transformer',\n",
    "    'train_from':'./model_files/pre_trained/abs_transformer/cnndm_baseline_best.pt',\n",
    "    'task': abs,\n",
    "\n",
    "    \n",
    "    'accum_count':5,\n",
    "    'batch_size':300,\n",
    "    'dec_dropout':0.1,\n",
    "    'lr':0.05,\n",
    "    'save_checkpoint_steps':15,\n",
    "    'sep_optim':False,\n",
    "    'train_steps':38001 + training_steps,\n",
    "    'use_bert_emb':True,\n",
    "    'warmup_steps':1,\n",
    "    'report_every':50,\n",
    "    'enc_hidden_size':512 ,\n",
    "'enc_layers':6,\n",
    "'enc_ff_size': 2048,\n",
    "'enc_dropout': 0.1,\n",
    "'dec_layers': 6,\n",
    "'dec_hidden_size': 512,\n",
    "'dec_ff_size':2048,\n",
    "'encoder': 'baseline'\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "args = Namespace(**abs_args_train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:27:21,908 INFO] Namespace(accum_count=5, alpha=0.95, batch_size=300, beam_size=5, bert_data_path='./data', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.1, dec_ff_size=2048, dec_heads=8, dec_hidden_size=512, dec_layers=6, enc_dropout=0.1, enc_ff_size=2048, enc_hidden_size=512, enc_layers=6, encoder='baseline', ext_dropout=0.1, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='./logs/train_abs_transformer', lr=0.05, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='train', model_path='./model_files/trained/abs_transformer', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='./results/trained/abs_transformer', save_checkpoint_steps=15, seed=666, sep_optim=False, share_emb=False, task=<built-in function abs>, temp_dir='./temp', test_all=False, test_batch_size=200, test_from='./model_files/trained/ext/model_step_18030.pt', test_start_from=-1, train_from='./model_files/pre_trained/abs_transformer/cnndm_baseline_best.pt', train_steps=38031, use_bert_emb=True, use_interval=True, user_interval=True, visible_gpus='-1', warmup_steps=1, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n",
      "[2020-05-09 19:27:21,909 INFO] Device ID -1\n",
      "[2020-05-09 19:27:21,910 INFO] Device cpu\n",
      "[2020-05-09 19:27:21,912 INFO] Loading checkpoint from ./model_files/pre_trained/abs_transformer/cnndm_baseline_best.pt\n",
      "[2020-05-09 19:27:22,317 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ./temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "[2020-05-09 19:27:22,318 INFO] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2020-05-09 19:27:22,395 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ./temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'opt', 'optims'])\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:27:26,314 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "[2020-05-09 19:27:26,349 INFO] * number of parameters: 75951418\n",
      "[2020-05-09 19:27:26,350 INFO] Start training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_rank 0\n",
      "pts ['./data/train.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:27:27,544 INFO] Loading train dataset from ./data/train.pt, number of examples: 2955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=38001, Train_steps=38031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:28:37,923 INFO] Saving checkpoint ./model_files/trained/abs_transformer/model_step_38010.pt\n",
      "[2020-05-09 19:30:26,395 INFO] Saving checkpoint ./model_files/trained/abs_transformer/model_step_38025.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts ['./data/train.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:31:11,705 INFO] Loading train dataset from ./data/train.pt, number of examples: 2955\n"
     ]
    }
   ],
   "source": [
    "train_abs_single(args, device_id=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:31:11,923 INFO] Loading checkpoint from ./model_files/trained/abs_transformer/model_step_38025.pt\n",
      "[2020-05-09 19:31:12,200 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ./temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "[2020-05-09 19:31:12,201 INFO] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2020-05-09 19:31:12,269 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ./temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(accum_count=5, alpha=0.95, batch_size=300, beam_size=5, bert_data_path='./data', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.1, dec_ff_size=2048, dec_heads=8, dec_hidden_size=512, dec_layers=6, enc_dropout=0.1, enc_ff_size=2048, enc_hidden_size=512, enc_layers=6, encoder='baseline', ext_dropout=0.1, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='./logs/abs_transformers_trained', lr=0.05, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='./model_files/trained/abs_transformer/', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='./results/trained/abs_transformer', save_checkpoint_steps=15, seed=666, sep_optim=False, share_emb=False, task='abs', temp_dir='./temp', test_all=False, test_batch_size=200, test_from='./model_files/trained/abs_transformer/model_step_38025.pt', test_start_from=-1, train_from='./model_files/pre_trained/abs_transformer/cnndm_baseline_best.pt', train_steps=38031, use_bert_emb=True, use_interval=True, user_interval=True, visible_gpus='-1', warmup_steps=1, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:31:16,037 INFO] Loading test dataset from ./data/test.pt, number of examples: 369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts ['./data/test.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 19:31:16,119 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "[2020-05-09 20:50:40,526 INFO] Calculating Rouge\n",
      "2020-05-09 20:50:40,572 [MainThread  ] [INFO ]  Writing summaries.\n",
      "[2020-05-09 20:50:40,572 INFO] Writing summaries.\n",
      "2020-05-09 20:50:40,574 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./temp/tmpn0wozr0p/system and model files to ./temp/tmpn0wozr0p/model.\n",
      "[2020-05-09 20:50:40,574 INFO] Processing summaries. Saving system files to ./temp/tmpn0wozr0p/system and model files to ./temp/tmpn0wozr0p/model.\n",
      "2020-05-09 20:50:40,577 [MainThread  ] [INFO ]  Processing files in ./temp/rouge-tmp-2020-05-09-20-50-40/candidate/.\n",
      "[2020-05-09 20:50:40,577 INFO] Processing files in ./temp/rouge-tmp-2020-05-09-20-50-40/candidate/.\n",
      "2020-05-09 20:50:40,625 [MainThread  ] [INFO ]  Saved processed files to ./temp/tmpn0wozr0p/system.\n",
      "[2020-05-09 20:50:40,625 INFO] Saved processed files to ./temp/tmpn0wozr0p/system.\n",
      "2020-05-09 20:50:40,627 [MainThread  ] [INFO ]  Processing files in ./temp/rouge-tmp-2020-05-09-20-50-40/reference/.\n",
      "[2020-05-09 20:50:40,627 INFO] Processing files in ./temp/rouge-tmp-2020-05-09-20-50-40/reference/.\n",
      "2020-05-09 20:50:40,678 [MainThread  ] [INFO ]  Saved processed files to ./temp/tmpn0wozr0p/model.\n",
      "[2020-05-09 20:50:40,678 INFO] Saved processed files to ./temp/tmpn0wozr0p/model.\n",
      "2020-05-09 20:50:40,684 [MainThread  ] [INFO ]  Written ROUGE configuration to ./temp/tmpish6tac7/rouge_conf.xml\n",
      "[2020-05-09 20:50:40,684 INFO] Written ROUGE configuration to ./temp/tmpish6tac7/rouge_conf.xml\n",
      "2020-05-09 20:50:40,685 [MainThread  ] [INFO ]  Running ROUGE with command perl ./ROUGE-1.5.5/ROUGE-1.5.5.pl -e ./ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./temp/tmpish6tac7/rouge_conf.xml\n",
      "[2020-05-09 20:50:40,685 INFO] Running ROUGE with command perl ./ROUGE-1.5.5/ROUGE-1.5.5.pl -e ./ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./temp/tmpish6tac7/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369\n",
      "369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 20:50:53,340 INFO] Rouges at step 0 \n",
      ">> ROUGE-F(1/2/3/l): 27.62/11.06/23.48\n",
      "ROUGE-R(1/2/3/l): 21.51/8.69/18.08\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.21512 (95%-conf.int. 0.20138 - 0.22990)\n",
      "1 ROUGE-1 Average_P: 0.54829 (95%-conf.int. 0.53100 - 0.56577)\n",
      "1 ROUGE-1 Average_F: 0.27616 (95%-conf.int. 0.26313 - 0.28898)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.08687 (95%-conf.int. 0.07851 - 0.09528)\n",
      "1 ROUGE-2 Average_P: 0.21848 (95%-conf.int. 0.20456 - 0.23297)\n",
      "1 ROUGE-2 Average_F: 0.11064 (95%-conf.int. 0.10257 - 0.11962)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.18084 (95%-conf.int. 0.17001 - 0.19243)\n",
      "1 ROUGE-L Average_P: 0.47653 (95%-conf.int. 0.45968 - 0.49282)\n",
      "1 ROUGE-L Average_F: 0.23477 (95%-conf.int. 0.22417 - 0.24531)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abs_args_dict = arg_params\n",
    "\n",
    "abs_args_dict.update({\n",
    "            'bert_data_path':'./data',\n",
    "    'log_file':'./logs/abs_transformers_trained',\n",
    "    'model_path':'./model_files/trained/abs_transformer/',\n",
    "    'result_path':'./results/trained/abs_transformer',\n",
    "    'test_from':f'./model_files/trained/abs_transformer/model_step_38025.pt',\n",
    "    'task':'abs',\n",
    "    'mode':'test',\n",
    "    \n",
    "    'batch_size':300,\n",
    "    'test_batch_size':200,\n",
    "    'max_pos':512,\n",
    "    'max_length':200,\n",
    "    'min_length':50,\n",
    "        \n",
    "    'sep_optim':False,\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "args = Namespace(**abs_args_dict)\n",
    "test_abs(args, device_id=-1, pt=args.test_from, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractive Summarization (BertExt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = 30\n",
    "abs_args_train_dict = arg_params\n",
    "\n",
    "abs_args_train_dict.update({\n",
    "    \n",
    "            'bert_data_path':'./data',\n",
    "    'log_file':'./logs/train_abs',\n",
    "    'mode':'train',\n",
    "    'model_path':'./model_files/trained/abs',\n",
    "    'result_path':'./results/trained/abs',\n",
    "    'train_from':'./model_files/pre_trained/abs_bertextabs/model_step_148000.pt',\n",
    "        'load_from_extractive':f'./model_files/trained/ext/model_step_{str(18001 + training_steps -1)}.pt',\n",
    "    'task': abs,\n",
    "    'save_checkpoint_steps':15,\n",
    "    'batch_size':300,\n",
    "    'train_steps':148001+training_steps,\n",
    "    'report_every':1,\n",
    "    'accum_count':5,\n",
    "    'warmup_steps_bert':1,\n",
    "    'warmup_steps_dec':1,\n",
    "  \n",
    "'dec_dropout':0.2,\n",
    "'sep_optim':True,\n",
    "'lr_bert':0.002,\n",
    "'lr_dec':0.2,\n",
    "'use_bert_emb':True,\n",
    "'use_interval':True,\n",
    "'max_pos':512\n",
    "})\n",
    "\n",
    "\n",
    "args = Namespace(**abs_args_train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 20:50:53,369 INFO] Namespace(accum_count=5, alpha=0.95, batch_size=300, beam_size=5, bert_data_path='./data', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=512, dec_layers=6, enc_dropout=0.1, enc_ff_size=2048, enc_hidden_size=512, enc_layers=6, encoder='baseline', ext_dropout=0.1, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='./model_files/trained/ext/model_step_18030.pt', log_file='./logs/train_abs', lr=0.05, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='train', model_path='./model_files/trained/abs', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='./results/trained/abs', save_checkpoint_steps=15, seed=666, sep_optim=True, share_emb=False, task=<built-in function abs>, temp_dir='./temp', test_all=False, test_batch_size=200, test_from='./model_files/trained/abs_transformer/model_step_38025.pt', test_start_from=-1, train_from='./model_files/pre_trained/abs_bertextabs/model_step_148000.pt', train_steps=148031, use_bert_emb=True, use_interval=True, user_interval=True, visible_gpus='-1', warmup_steps=1, warmup_steps_bert=1, warmup_steps_dec=1, world_size=1)\n",
      "[2020-05-09 20:50:53,370 INFO] Device ID -1\n",
      "[2020-05-09 20:50:53,371 INFO] Device cpu\n",
      "[2020-05-09 20:50:53,373 INFO] Loading checkpoint from ./model_files/pre_trained/abs_bertextabs/model_step_148000.pt\n",
      "[2020-05-09 20:50:54,141 INFO] Loading bert from extractive model ./model_files/trained/ext/model_step_18030.pt\n",
      "[2020-05-09 20:50:54,863 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ./temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "[2020-05-09 20:50:54,864 INFO] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2020-05-09 20:50:54,939 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ./temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "[2020-05-09 20:50:59,272 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "[2020-05-09 20:50:59,310 INFO] * number of parameters: 180222522\n",
      "[2020-05-09 20:50:59,310 INFO] Start training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_rank 0\n",
      "pts ['./data/train.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 20:51:00,531 INFO] Loading train dataset from ./data/train.pt, number of examples: 2955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=148001, Train_steps=148031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 20:51:17,324 INFO] Step 148001/148031; acc:  26.90; ppl: 87.66; xent: 4.47; lr: 0.00000520;   0/ 37 tok/s;     17 sec\n",
      "[2020-05-09 20:51:32,878 INFO] Step 148002/148031; acc:  34.17; ppl: 61.84; xent: 4.12; lr: 0.00000520;   0/ 34 tok/s;     32 sec\n",
      "[2020-05-09 20:51:49,484 INFO] Step 148003/148031; acc:  23.77; ppl: 105.11; xent: 4.65; lr: 0.00000520;   0/ 40 tok/s;     49 sec\n",
      "[2020-05-09 20:52:06,043 INFO] Step 148004/148031; acc:  17.99; ppl: 188.99; xent: 5.24; lr: 0.00000520;   0/ 42 tok/s;     66 sec\n",
      "[2020-05-09 20:52:22,765 INFO] Step 148005/148031; acc:  29.32; ppl: 83.67; xent: 4.43; lr: 0.00000520;   0/ 36 tok/s;     82 sec\n",
      "[2020-05-09 20:52:22,771 INFO] Saving checkpoint ./model_files/trained/abs/model_step_148005.pt\n",
      "[2020-05-09 20:52:39,505 INFO] Step 148006/148031; acc:  31.46; ppl: 65.89; xent: 4.19; lr: 0.00000520;   0/ 34 tok/s;     99 sec\n",
      "[2020-05-09 20:52:55,507 INFO] Step 148007/148031; acc:  29.44; ppl: 69.42; xent: 4.24; lr: 0.00000520;   0/ 38 tok/s;    115 sec\n",
      "[2020-05-09 20:53:12,059 INFO] Step 148008/148031; acc:  26.19; ppl: 86.04; xent: 4.45; lr: 0.00000520;   0/ 42 tok/s;    132 sec\n",
      "[2020-05-09 20:53:27,800 INFO] Step 148009/148031; acc:  43.58; ppl: 25.85; xent: 3.25; lr: 0.00000520;   0/ 33 tok/s;    147 sec\n",
      "[2020-05-09 20:53:44,044 INFO] Step 148010/148031; acc:  27.73; ppl: 63.38; xent: 4.15; lr: 0.00000520;   0/ 40 tok/s;    164 sec\n",
      "[2020-05-09 20:54:00,371 INFO] Step 148011/148031; acc:  32.26; ppl: 52.07; xent: 3.95; lr: 0.00000520;   0/ 36 tok/s;    180 sec\n",
      "[2020-05-09 20:54:16,815 INFO] Step 148012/148031; acc:  27.86; ppl: 78.38; xent: 4.36; lr: 0.00000520;   0/ 41 tok/s;    196 sec\n",
      "[2020-05-09 20:54:32,934 INFO] Step 148013/148031; acc:  30.73; ppl: 54.83; xent: 4.00; lr: 0.00000520;   0/ 37 tok/s;    212 sec\n",
      "[2020-05-09 20:54:49,333 INFO] Step 148014/148031; acc:  24.64; ppl: 89.39; xent: 4.49; lr: 0.00000520;   0/ 42 tok/s;    229 sec\n",
      "[2020-05-09 20:55:05,970 INFO] Step 148015/148031; acc:  37.99; ppl: 35.37; xent: 3.57; lr: 0.00000520;   0/ 42 tok/s;    245 sec\n",
      "[2020-05-09 20:55:22,661 INFO] Step 148016/148031; acc:  30.17; ppl: 57.88; xent: 4.06; lr: 0.00000520;   0/ 35 tok/s;    262 sec\n",
      "[2020-05-09 20:55:38,907 INFO] Step 148017/148031; acc:  31.08; ppl: 65.68; xent: 4.18; lr: 0.00000520;   0/ 43 tok/s;    278 sec\n",
      "[2020-05-09 20:55:55,101 INFO] Step 148018/148031; acc:  36.95; ppl: 42.84; xent: 3.76; lr: 0.00000520;   0/ 40 tok/s;    295 sec\n",
      "[2020-05-09 20:56:11,758 INFO] Step 148019/148031; acc:  32.09; ppl: 57.66; xent: 4.05; lr: 0.00000520;   0/ 42 tok/s;    311 sec\n",
      "[2020-05-09 20:56:28,026 INFO] Step 148020/148031; acc:  29.21; ppl: 57.24; xent: 4.05; lr: 0.00000520;   0/ 43 tok/s;    327 sec\n",
      "[2020-05-09 20:56:28,032 INFO] Saving checkpoint ./model_files/trained/abs/model_step_148020.pt\n",
      "[2020-05-09 20:56:45,144 INFO] Step 148021/148031; acc:  29.14; ppl: 70.98; xent: 4.26; lr: 0.00000520;   0/ 37 tok/s;    345 sec\n",
      "[2020-05-09 20:57:01,821 INFO] Step 148022/148031; acc:  34.16; ppl: 50.45; xent: 3.92; lr: 0.00000520;   0/ 41 tok/s;    361 sec\n",
      "[2020-05-09 20:57:17,904 INFO] Step 148023/148031; acc:  34.24; ppl: 46.76; xent: 3.85; lr: 0.00000520;   0/ 43 tok/s;    377 sec\n",
      "[2020-05-09 20:57:34,473 INFO] Step 148024/148031; acc:  28.23; ppl: 51.65; xent: 3.94; lr: 0.00000520;   0/ 38 tok/s;    394 sec\n",
      "[2020-05-09 20:57:51,531 INFO] Step 148025/148031; acc:  35.97; ppl: 42.07; xent: 3.74; lr: 0.00000520;   0/ 41 tok/s;    411 sec\n",
      "[2020-05-09 20:58:07,987 INFO] Step 148026/148031; acc:  29.21; ppl: 62.05; xent: 4.13; lr: 0.00000520;   0/ 42 tok/s;    427 sec\n",
      "[2020-05-09 20:58:24,176 INFO] Step 148027/148031; acc:  33.39; ppl: 45.66; xent: 3.82; lr: 0.00000520;   0/ 36 tok/s;    444 sec\n",
      "[2020-05-09 20:58:40,212 INFO] Step 148028/148031; acc:  40.47; ppl: 32.91; xent: 3.49; lr: 0.00000520;   0/ 37 tok/s;    460 sec\n",
      "[2020-05-09 20:58:56,696 INFO] Step 148029/148031; acc:  27.98; ppl: 64.00; xent: 4.16; lr: 0.00000520;   0/ 40 tok/s;    476 sec\n",
      "[2020-05-09 20:59:13,612 INFO] Step 148030/148031; acc:  30.65; ppl: 54.79; xent: 4.00; lr: 0.00000520;   0/ 41 tok/s;    493 sec\n",
      "[2020-05-09 20:59:30,182 INFO] Step 148031/148031; acc:  34.18; ppl: 42.14; xent: 3.74; lr: 0.00000520;   0/ 40 tok/s;    510 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts ['./data/train.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 20:59:31,727 INFO] Loading train dataset from ./data/train.pt, number of examples: 2955\n"
     ]
    }
   ],
   "source": [
    "train_abs_single(args, device_id=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 21:03:06,251 INFO] Loading checkpoint from ./model_files/trained/abs/model_step_148020.pt\n",
      "[2020-05-09 21:03:06,758 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ./temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "[2020-05-09 21:03:06,760 INFO] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2020-05-09 21:03:06,835 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ./temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(accum_count=5, alpha=0.95, batch_size=300, beam_size=5, bert_data_path='./data', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.1, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.1, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='./model_files/trained/ext/model_step_18030.pt', log_file='./logs/abs_bertextabs_trained', lr=0.05, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='./model_files/trained/abs_bertextabs/', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='./results/trained/abs_bertextabs', save_checkpoint_steps=15, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='./temp', test_all=False, test_batch_size=200, test_from='./model_files/trained/abs/model_step_148020.pt', test_start_from=-1, train_from='./model_files/pre_trained/abs_bertextabs/model_step_148000.pt', train_steps=148031, use_bert_emb=True, use_interval=True, user_interval=True, visible_gpus='-1', warmup_steps=1, warmup_steps_bert=1, warmup_steps_dec=1, world_size=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 21:03:10,379 INFO] Loading test dataset from ./data/test.pt, number of examples: 369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts ['./data/test.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 21:03:10,457 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "[2020-05-09 22:23:27,746 INFO] Calculating Rouge\n",
      "2020-05-09 22:23:27,793 [MainThread  ] [INFO ]  Writing summaries.\n",
      "[2020-05-09 22:23:27,793 INFO] Writing summaries.\n",
      "2020-05-09 22:23:27,795 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./temp/tmph8zoxwkw/system and model files to ./temp/tmph8zoxwkw/model.\n",
      "[2020-05-09 22:23:27,795 INFO] Processing summaries. Saving system files to ./temp/tmph8zoxwkw/system and model files to ./temp/tmph8zoxwkw/model.\n",
      "2020-05-09 22:23:27,797 [MainThread  ] [INFO ]  Processing files in ./temp/rouge-tmp-2020-05-09-22-23-27/candidate/.\n",
      "[2020-05-09 22:23:27,797 INFO] Processing files in ./temp/rouge-tmp-2020-05-09-22-23-27/candidate/.\n",
      "2020-05-09 22:23:27,845 [MainThread  ] [INFO ]  Saved processed files to ./temp/tmph8zoxwkw/system.\n",
      "[2020-05-09 22:23:27,845 INFO] Saved processed files to ./temp/tmph8zoxwkw/system.\n",
      "2020-05-09 22:23:27,846 [MainThread  ] [INFO ]  Processing files in ./temp/rouge-tmp-2020-05-09-22-23-27/reference/.\n",
      "[2020-05-09 22:23:27,846 INFO] Processing files in ./temp/rouge-tmp-2020-05-09-22-23-27/reference/.\n",
      "2020-05-09 22:23:27,902 [MainThread  ] [INFO ]  Saved processed files to ./temp/tmph8zoxwkw/model.\n",
      "[2020-05-09 22:23:27,902 INFO] Saved processed files to ./temp/tmph8zoxwkw/model.\n",
      "2020-05-09 22:23:27,908 [MainThread  ] [INFO ]  Written ROUGE configuration to ./temp/tmplk3d3mhj/rouge_conf.xml\n",
      "[2020-05-09 22:23:27,908 INFO] Written ROUGE configuration to ./temp/tmplk3d3mhj/rouge_conf.xml\n",
      "2020-05-09 22:23:27,909 [MainThread  ] [INFO ]  Running ROUGE with command perl ./ROUGE-1.5.5/ROUGE-1.5.5.pl -e ./ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./temp/tmplk3d3mhj/rouge_conf.xml\n",
      "[2020-05-09 22:23:27,909 INFO] Running ROUGE with command perl ./ROUGE-1.5.5/ROUGE-1.5.5.pl -e ./ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./temp/tmplk3d3mhj/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369\n",
      "369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-09 22:23:42,309 INFO] Rouges at step 0 \n",
      ">> ROUGE-F(1/2/3/l): 29.28/10.66/24.97\n",
      "ROUGE-R(1/2/3/l): 24.29/8.91/20.57\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.24294 (95%-conf.int. 0.22840 - 0.25752)\n",
      "1 ROUGE-1 Average_P: 0.50696 (95%-conf.int. 0.49036 - 0.52454)\n",
      "1 ROUGE-1 Average_F: 0.29275 (95%-conf.int. 0.28149 - 0.30372)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.08911 (95%-conf.int. 0.08084 - 0.09830)\n",
      "1 ROUGE-2 Average_P: 0.18710 (95%-conf.int. 0.17395 - 0.20229)\n",
      "1 ROUGE-2 Average_F: 0.10662 (95%-conf.int. 0.09880 - 0.11491)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.20565 (95%-conf.int. 0.19380 - 0.21763)\n",
      "1 ROUGE-L Average_P: 0.43912 (95%-conf.int. 0.42397 - 0.45664)\n",
      "1 ROUGE-L Average_F: 0.24970 (95%-conf.int. 0.24002 - 0.25936)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abs_args_dict = arg_params\n",
    "\n",
    "abs_args_dict.update({\n",
    "            'bert_data_path':'./data',\n",
    "    'log_file':'./logs/abs_bertextabs_trained',\n",
    "    'model_path':'./model_files/trained/abs_bertextabs/',\n",
    "    'result_path':'./results/trained/abs_bertextabs',\n",
    "    'test_from':f'./model_files/trained/abs/model_step_148020.pt',\n",
    "    'task':'abs',\n",
    "    'mode':'test',\n",
    "    'batch_size':300,\n",
    "    'test_batch_size':200,\n",
    "    'max_pos':512,\n",
    "    'max_length':200,\n",
    "    'alpha': 0.95,\n",
    "    'min_length':50,\n",
    "        \n",
    "    'sep_optim':True,\n",
    "    'user_interval':True\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "args = Namespace(**abs_args_dict)\n",
    "test_abs(args, device_id=-1, pt=args.test_from, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matchsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.get_candidate import get_candidates_mp\n",
    "from argparse import Namespace\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "from preprocess.train_matching import test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {'data_path':'matchsumm_data/match_summ_sample.json',\n",
    "            'index_path':'matchsumm_data/sentence_id.json',\n",
    "            'write_path':'data/test_CNNDM_bert.jsonl',\n",
    "            'tokenizer':'bert'}\n",
    "args = Namespace(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "shutil.rmtree('./temp') if os.path.isdir('./temp') else None\n",
    "get_candidates_mp(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {'mode':'test',\n",
    "'encoder':'bert',\n",
    "'save_path':'matchsumm_models/',\n",
    "            'candidate_num':20,\n",
    "            'gpus':0,\n",
    "            'encoder':'bert'}\n",
    "args = Namespace(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading datasets !!!\n",
      "Finished in 0:00:00.152812\n",
      "Information of dataset is:\n",
      "In total 1 datasets:\n",
      "\ttest has 369 instances.\n",
      "\n",
      "Current model is MatchSum_cnndm_bert.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/case_law_g45/venv/lib64/python3.6/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/data/case_law_g45/venv/lib64/python3.6/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertEmbeddings' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/data/case_law_g45/venv/lib64/python3.6/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.normalization.LayerNorm' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/data/case_law_g45/venv/lib64/python3.6/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/data/case_law_g45/venv/lib64/python3.6/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertSelfAttention' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/data/case_law_g45/venv/lib64/python3.6/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/data/case_law_g45/venv/lib64/python3.6/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Tanh' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369/369 (100.00%) decoded in 0:11:54 seconds\n",
      "Start writing files !!!\n",
      "Start evaluating ROUGE score !!!\n",
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.33545 (95%-conf.int. 0.31925 - 0.35333)\n",
      "1 ROUGE-1 Average_P: 0.79428 (95%-conf.int. 0.78238 - 0.80488)\n",
      "1 ROUGE-1 Average_F: 0.44129 (95%-conf.int. 0.42646 - 0.45706)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.24185 (95%-conf.int. 0.22784 - 0.25637)\n",
      "1 ROUGE-2 Average_P: 0.58127 (95%-conf.int. 0.56360 - 0.59731)\n",
      "1 ROUGE-2 Average_F: 0.31916 (95%-conf.int. 0.30535 - 0.33325)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.31068 (95%-conf.int. 0.29551 - 0.32740)\n",
      "1 ROUGE-L Average_P: 0.74182 (95%-conf.int. 0.72769 - 0.75541)\n",
      "1 ROUGE-L Average_F: 0.40957 (95%-conf.int. 0.39584 - 0.42462)\n",
      "\n",
      "Evaluate data in 730.31 seconds!\n",
      "[tester] \n",
      "MatchRougeMetric: ROUGE-1=0.44129, ROUGE-2=0.31916, ROUGE-L=0.40957\n",
      "Current model is MatchSum_cnndm_roberta.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/case_law_g45/venv/lib64/python3.6/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'transformers.modeling_roberta.RobertaEmbeddings' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369/369 (100.00%) decoded in 0:11:48 seconds\n",
      "Start writing files !!!\n",
      "Start evaluating ROUGE score !!!\n",
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.43630 (95%-conf.int. 0.41940 - 0.45346)\n",
      "1 ROUGE-1 Average_P: 0.77631 (95%-conf.int. 0.76389 - 0.78864)\n",
      "1 ROUGE-1 Average_F: 0.52943 (95%-conf.int. 0.51599 - 0.54246)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.32228 (95%-conf.int. 0.30765 - 0.33644)\n",
      "1 ROUGE-2 Average_P: 0.57995 (95%-conf.int. 0.56391 - 0.59588)\n",
      "1 ROUGE-2 Average_F: 0.39246 (95%-conf.int. 0.37956 - 0.40653)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.40682 (95%-conf.int. 0.39012 - 0.42230)\n",
      "1 ROUGE-L Average_P: 0.72883 (95%-conf.int. 0.71508 - 0.74245)\n",
      "1 ROUGE-L Average_F: 0.49484 (95%-conf.int. 0.48198 - 0.50785)\n",
      "\n",
      "Evaluate data in 727.33 seconds!\n",
      "[tester] \n",
      "MatchRougeMetric: ROUGE-1=0.52943, ROUGE-2=0.39246, ROUGE-L=0.49484\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree('./temp') if os.path.isdir('./temp') else None\n",
    "shutil.rmtree('data/result/') if os.path.isdir('data/result/') else None\n",
    "test_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
