{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Data Science 2: Advanced Topics in Data Science \n",
    "\n",
    "##  Final Project: Milestone 3 - Final Project [70 pts]\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2020**<br/>\n",
    "**Group Members**: Fernando Medeiros, Mohammed Gufran Pathan, and Prerna Aggarwal<br/>\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML, display\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theme\"> Final Deliverables </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Code Report:** You are expected to submit the code you developed as part of the course project. The commented code should be provided in report format. This means that each group in a Jupyter notebook should explain—in a clean and concise report fashion—how they proceeded at every step and coding/methodology  choices . The code report should have a structure that consists of an introduction, body and conclusion.\n",
    "1. **Ignite Talk:** You will present the talk on 5/11, 5/12, or 5/13. Details to come for Ignite Talk guidelines.\n",
    "\n",
    "[Final Project Guidelines](https://docs.google.com/document/d/1Zhmm9JP4FGQBi5abFiM22e5iXYo_rr7i_vbpW0-xt8A/edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreSumm\n",
    "\n",
    "**Source**:\n",
    "\n",
    "Code: https://github.com/nlpyang/PreSumm/\n",
    "\n",
    "Paper: https://arxiv.org/abs/1908.08345\n",
    "\n",
    "#### Dependencies\n",
    "\n",
    "**Libraries**: \n",
    "\n",
    "Torch 1.1.0 (download instructions from https://pytorch.org/get-started/previous-versions/)\n",
    "\n",
    "fastNLP (to install use ```pip install fastNLP```)\n",
    "\n",
    "pyrouge (to install use ```pip install pyrouge```)\n",
    "\n",
    "pytorch-transformers (use ```pip install pytorch-transformers``` to import BertTokenizer from others.tokenization)\n",
    "\n",
    "rouge (to install use ```pip install rouge```)\n",
    "\n",
    "transformers\n",
    "\n",
    "```git clone https://github.com/huggingface/transformers\n",
    "cd transformers\n",
    "pip install .```\n",
    "\n",
    "**Stanford CoreNLP**\n",
    "\n",
    "We will need Stanford CoreNLP to tokenize the data. Download it [here](https://stanfordnlp.github.io/CoreNLP/) and unzip it. Then add the following command to your bash_profile:\n",
    "```\n",
    "export CLASSPATH=/path/to/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar\n",
    "```\n",
    "replacing `/path/to/` with the path to where you saved the `stanford-corenlp-full-2017-06-09` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baisc Python Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import timeit\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "# Project Python Lybraries\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import lxml\n",
    "import lzma\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import queue\n",
    "import re\n",
    "import subprocess as sp\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "\n",
    "from argparse import Namespace\n",
    "from bs4 import BeautifulSoup\n",
    "from cytoolz import curry\n",
    "from datetime import timedelta\n",
    "from fastNLP.core.callback import SaveModelCallback\n",
    "from fastNLP.core.tester import Tester\n",
    "from fastNLP.core.trainer import Trainer\n",
    "from itertools import combinations\n",
    "from os.path import join, exists\n",
    "from pandas.io.json import json_normalize\n",
    "from pyrouge import Rouge155\n",
    "from pyrouge.utils import log\n",
    "from others.tokenization import BertTokenizer\n",
    "from time import time\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.optim import Adam\n",
    "from transformers import BertModel, RobertaModel\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "\n",
    "#from model import MatchSum\n",
    "#from callback import MyCallback\n",
    "#from dataloader import MatchSumPipe\n",
    "#from metrics import MarginRankingLoss, ValidMetric, MatchRougeMetric\n",
    "#from utils import read_jsonl, get_data_path, get_result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-07 21:05:39,026 [MainThread  ] [INFO ]  Set ROUGE home directory to ./ROUGE-1.5.5.\n",
      "INFO:global:Set ROUGE home directory to ./ROUGE-1.5.5.\n"
     ]
    }
   ],
   "source": [
    "r = Rouge155('./ROUGE-1.5.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening data for north_carolina.xz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fmedeiros/anaconda3/envs/cs109b_FP/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "base_path = \"./data/text\"\n",
    "state = 'north_carolina.xz'\n",
    "f = lzma.open(os.path.join(base_path, state), \"rb\")\n",
    "state_data = f.readlines()\n",
    "f.close()\n",
    "data_json = [json.loads(line) for line in state_data]\n",
    "print(f'Flattening data for {state}')\n",
    "data = json_normalize(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['decision_date_p'] = pd.to_datetime(data.decision_date,errors='coerce')\n",
    "data['decision_year'] = data.decision_date_p.dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(raw_path,save_path):\n",
    "    stories_dir = os.path.abspath(raw_path)\n",
    "    tokenized_stories_dir = os.path.abspath(save_path)\n",
    "\n",
    "    print(\"Preparing to tokenize %s to %s...\" % (stories_dir, tokenized_stories_dir))\n",
    "    stories = os.listdir(stories_dir)\n",
    "    # make IO list file\n",
    "    print(\"Making list of files to tokenize...\")\n",
    "    with open(\"mapping_for_corenlp.txt\", \"w\") as f:\n",
    "        for s in stories:\n",
    "            f.write(\"%s\\n\" % (os.path.join(stories_dir, s)))\n",
    "    command = ['java', 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize,ssplit',\n",
    "               '-ssplit.newlineIsSentenceBreak', 'always', '-filelist', 'mapping_for_corenlp.txt', '-outputFormat',\n",
    "               'json', '-outputDirectory', tokenized_stories_dir]\n",
    "    print(\"Tokenizing %i files in %s and saving in %s...\" % (len(stories), stories_dir, tokenized_stories_dir))\n",
    "    subprocess.call(command)\n",
    "    print(\"Stanford CoreNLP Tokenizer has finished.\")\n",
    "    os.remove(\"mapping_for_corenlp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = data.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in sample_data.iterrows():\n",
    "    caseid = row[1].id\n",
    "    markup = row[1]['casebody.data']\n",
    "    soup = BeautifulSoup(markup, \"xml\")\n",
    "    opinion = soup.find_all('opinion')[0]\n",
    "    opinion_text = opinion.getText()\n",
    "    headnotes = ' '.join([headnote.getText() for headnote in soup.find_all('headnotes')])\n",
    "    \n",
    "    with open(f'presumm_data/parsed_text/opinions/{caseid}.txt','w',encoding='utf-8') as f:\n",
    "        f.write(opinion_text)\n",
    "    \n",
    "    with open(f'presumm_data/parsed_text/headnotes/{caseid}.txt','w',encoding='utf-8') as f:\n",
    "        f.write(headnotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to tokenize /Users/fmedeiros/Documents/My Education/Harvard Extension School/case_law_g45/presumm_data/parsed_text/opinions to /Users/fmedeiros/Documents/My Education/Harvard Extension School/case_law_g45/presumm_data/tokenized_text/opinions...\n",
      "Making list of files to tokenize...\n",
      "Tokenizing 10 files in /Users/fmedeiros/Documents/My Education/Harvard Extension School/case_law_g45/presumm_data/parsed_text/opinions and saving in /Users/fmedeiros/Documents/My Education/Harvard Extension School/case_law_g45/presumm_data/tokenized_text/opinions...\n",
      "Stanford CoreNLP Tokenizer has finished.\n"
     ]
    }
   ],
   "source": [
    "parsed_opinions_path = 'presumm_data/parsed_text/opinions'\n",
    "tokenized_opinions_path = 'presumm_data/tokenized_text/opinions'\n",
    "tokenize(parsed_opinions_path, tokenized_opinions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to tokenize /Users/fmedeiros/Documents/My Education/Harvard Extension School/case_law_g45/presumm_data/parsed_text/headnotes to /Users/fmedeiros/Documents/My Education/Harvard Extension School/case_law_g45/presumm_data/tokenized_text/headnotes...\n",
      "Making list of files to tokenize...\n",
      "Tokenizing 10 files in /Users/fmedeiros/Documents/My Education/Harvard Extension School/case_law_g45/presumm_data/parsed_text/headnotes and saving in /Users/fmedeiros/Documents/My Education/Harvard Extension School/case_law_g45/presumm_data/tokenized_text/headnotes...\n",
      "Stanford CoreNLP Tokenizer has finished.\n"
     ]
    }
   ],
   "source": [
    "parsed_headnotes_path = 'presumm_data/parsed_text/headnotes'\n",
    "tokenized_headnotes_path = 'presumm_data/tokenized_text/headnotes'\n",
    "tokenize(parsed_headnotes_path, tokenized_headnotes_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMAP = {\"-lrb-\": \"(\", \"-rrb-\": \")\", \"-lcb-\": \"{\", \"-rcb-\": \"}\",\n",
    "         \"-lsb-\": \"[\", \"-rsb-\": \"]\", \"``\": '\"', \"''\": '\"'}\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    return re.sub(\n",
    "        r\"-lrb-|-rrb-|-lcb-|-rcb-|-lsb-|-rsb-|``|''\",\n",
    "        lambda m: REMAP.get(m.group()), x)\n",
    "\n",
    "def load_json(case_id):\n",
    "    source = []\n",
    "    tgt = []\n",
    "    source_path = os.path.join('presumm_data/tokenized_text/opinions',f'{case_id}.txt.json')\n",
    "    target_path = os.path.join('presumm_data/tokenized_text/headnotes',f'{case_id}.txt.json')\n",
    "    for sent in json.load(open(source_path,encoding='utf-8'))['sentences']:\n",
    "        tokens = [t['word'] for t in sent['tokens']]\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        source.append(tokens)\n",
    "    for sent in json.load(open(target_path,encoding='utf-8'))['sentences']:\n",
    "        tokens = [t['word'] for t in sent['tokens']]\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        tgt.append(tokens)\n",
    "\n",
    "    source = [clean(' '.join(sent)).split() for sent in source]\n",
    "    tgt = [clean(' '.join(sent)).split() for sent in tgt]\n",
    "    return source, tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _get_ngrams(n, text):\n",
    "    \"\"\"Calcualtes n-grams.\n",
    "\n",
    "    Args:\n",
    "      n: which n-grams to calculate\n",
    "      text: An array of tokens\n",
    "\n",
    "    Returns:\n",
    "      A set of n-grams\n",
    "    \"\"\"\n",
    "    ngram_set = set()\n",
    "    text_length = len(text)\n",
    "    max_index_ngram_start = text_length - n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set.add(tuple(text[i:i + n]))\n",
    "    return ngram_set\n",
    "\n",
    "\n",
    "def _get_word_ngrams(n, sentences):\n",
    "    \"\"\"Calculates word n-grams for multiple sentences.\n",
    "    \"\"\"\n",
    "    assert len(sentences) > 0\n",
    "    assert n > 0\n",
    "\n",
    "    # words = _split_into_words(sentences)\n",
    "\n",
    "    words = sum(sentences, [])\n",
    "    # words = [w for w in words if w not in stopwords]\n",
    "    return _get_ngrams(n, words)\n",
    "\n",
    "\n",
    "def cal_rouge(evaluated_ngrams, reference_ngrams):\n",
    "    reference_count = len(reference_ngrams)\n",
    "    evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    if evaluated_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlapping_count / evaluated_count\n",
    "\n",
    "    if reference_count == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlapping_count / reference_count\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "    return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
    "\n",
    "\n",
    "def greedy_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "   \n",
    "    max_rouge = 0.0\n",
    "    abstract = sum(abstract_sent_list, [])\n",
    "    #abstract = abstract_sent_list\n",
    "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
    "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    #print(evaluated_1grams)\n",
    "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
    "\n",
    "    selected = []\n",
    "\n",
    "    for s in range(summary_size):\n",
    "        cur_max_rouge = max_rouge\n",
    "        cur_id = -1\n",
    "        \n",
    "        for i in range(len(sents)):\n",
    "            if (i in selected):\n",
    "                continue\n",
    "                \n",
    "            c = selected + [i]\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "            rouge_score = rouge_1 + rouge_2           \n",
    "            if rouge_score > cur_max_rouge:\n",
    "                cur_max_rouge = rouge_score\n",
    "                cur_id = i\n",
    "        if (cur_id == -1):\n",
    "            return sorted(selected)\n",
    "        selected.append(cur_id)\n",
    "        max_rouge = cur_max_rouge\n",
    "    \n",
    "    \n",
    "    return sorted(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_src_nsents =10000\n",
    "class BertData():\n",
    "    def __init__(self, min_src_ntokens_per_sent=5,\n",
    "                max_src_ntokens_per_sent=200,\n",
    "                max_src_nsents=max_src_nsents,\n",
    "                min_src_nsents=1,\n",
    "                max_tgt_ntokens=500,\n",
    "                min_tgt_ntokens=5):\n",
    "        self.min_src_ntokens_per_sent = min_src_ntokens_per_sent\n",
    "        self.max_src_ntokens_per_sent = max_src_ntokens_per_sent\n",
    "        self.max_src_nsents = max_src_nsents\n",
    "        self.min_src_nsents = min_src_nsents\n",
    "        self.max_tgt_ntokens = max_tgt_ntokens\n",
    "        self.min_tgt_ntokens = min_tgt_ntokens\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.tgt_bos = '[unused0]'\n",
    "        self.tgt_eos = '[unused1]'\n",
    "        self.tgt_sent_split = '[unused2]'\n",
    "        self.sep_vid = self.tokenizer.vocab[self.sep_token]\n",
    "        self.cls_vid = self.tokenizer.vocab[self.cls_token]\n",
    "        self.pad_vid = self.tokenizer.vocab[self.pad_token]\n",
    "\n",
    "    def preprocess(self, src, tgt, sent_labels, use_bert_basic_tokenizer=False, is_test=False):\n",
    "\n",
    "        if ((not is_test) and len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [' '.join(s) for s in src]\n",
    "\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > self.min_src_ntokens_per_sent)]\n",
    "\n",
    "        _sent_labels = [0] * len(src)\n",
    "        for l in sent_labels:\n",
    "            _sent_labels[l] = 1\n",
    "\n",
    "        src = [src[i][:self.max_src_ntokens_per_sent] for i in idxs]\n",
    "        sent_labels = [_sent_labels[i] for i in idxs]\n",
    "        src = src[:self.max_src_nsents]\n",
    "        sent_labels = sent_labels[:self.max_src_nsents]\n",
    "\n",
    "        if ((not is_test) and len(src) < self.min_src_nsents):\n",
    "            return None\n",
    "\n",
    "        src_txt = [' '.join(sent) for sent in src]\n",
    "        text = ' {} {} '.format(self.sep_token, self.cls_token).join(src_txt)\n",
    "\n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        src_subtokens = [self.cls_token] + src_subtokens + [self.sep_token]\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        sent_labels = sent_labels[:len(cls_ids)]\n",
    "\n",
    "        tgt_subtokens_str = '[unused0] ' + ' [unused2] '.join(\n",
    "            [' '.join(self.tokenizer.tokenize(' '.join(tt), use_bert_basic_tokenizer=use_bert_basic_tokenizer)) for tt in tgt]) + ' [unused1]'\n",
    "        tgt_subtoken = tgt_subtokens_str.split()[:self.max_tgt_ntokens]\n",
    "        if ((not is_test) and len(tgt_subtoken) < self.min_tgt_ntokens):\n",
    "            return None\n",
    "\n",
    "        tgt_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(tgt_subtoken)\n",
    "\n",
    "        tgt_txt = '<q>'.join([' '.join(tt) for tt in tgt])\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "\n",
    "        return src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "case_ids = ['1268383','11272108','11272573','11272694','11273468',\n",
    "            '11273534','11274033','11274050','11645357','11956941']\n",
    "for case_id in case_ids:\n",
    "    source, tgt = load_json(case_id)\n",
    "    sent_labels = greedy_selection(source[:max_src_nsents], tgt, 5)\n",
    "    source = [' '.join(s).lower().split() for s in source]\n",
    "    tgt = [' '.join(s).lower().split() for s in tgt]\n",
    "    bert = BertData()\n",
    "    b_data = bert.preprocess(source, tgt, sent_labels, use_bert_basic_tokenizer=True,\n",
    "                                     is_test=False)\n",
    "    if b_data is not None:\n",
    "        src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
    "        b_data_dict = {\"src\": src_subtoken_idxs, \"tgt\": tgt_subtoken_idxs,\n",
    "                               \"src_sent_labels\": sent_labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "                               'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
    "        datasets.append(b_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(datasets, 'presumm_data/bert_sample.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = torch.load('presumm_data/bert_sample.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df = torch.load('data/bert_data_cnndm_final/cnndm.test.0.bert.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': [101,\n",
       "  1037,\n",
       "  2118,\n",
       "  1997,\n",
       "  5947,\n",
       "  3076,\n",
       "  2038,\n",
       "  2351,\n",
       "  3053,\n",
       "  2093,\n",
       "  2706,\n",
       "  2044,\n",
       "  1037,\n",
       "  2991,\n",
       "  1999,\n",
       "  4199,\n",
       "  1999,\n",
       "  1037,\n",
       "  6878,\n",
       "  13742,\n",
       "  2886,\n",
       "  1999,\n",
       "  4199,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  4080,\n",
       "  9587,\n",
       "  29076,\n",
       "  1010,\n",
       "  2322,\n",
       "  1010,\n",
       "  2013,\n",
       "  8904,\n",
       "  3449,\n",
       "  9644,\n",
       "  1010,\n",
       "  4307,\n",
       "  1010,\n",
       "  2018,\n",
       "  2069,\n",
       "  2074,\n",
       "  3369,\n",
       "  2005,\n",
       "  1037,\n",
       "  13609,\n",
       "  2565,\n",
       "  1999,\n",
       "  3304,\n",
       "  2043,\n",
       "  1996,\n",
       "  5043,\n",
       "  3047,\n",
       "  1999,\n",
       "  2254,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  2002,\n",
       "  2001,\n",
       "  10583,\n",
       "  2067,\n",
       "  2000,\n",
       "  3190,\n",
       "  3081,\n",
       "  2250,\n",
       "  10771,\n",
       "  2006,\n",
       "  2233,\n",
       "  2322,\n",
       "  1010,\n",
       "  2021,\n",
       "  2002,\n",
       "  2351,\n",
       "  2006,\n",
       "  4465,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  4080,\n",
       "  9587,\n",
       "  29076,\n",
       "  1010,\n",
       "  2322,\n",
       "  1010,\n",
       "  2013,\n",
       "  8904,\n",
       "  3449,\n",
       "  9644,\n",
       "  1010,\n",
       "  4307,\n",
       "  1010,\n",
       "  1037,\n",
       "  2118,\n",
       "  1997,\n",
       "  5947,\n",
       "  3076,\n",
       "  2038,\n",
       "  2351,\n",
       "  3053,\n",
       "  2093,\n",
       "  2706,\n",
       "  2044,\n",
       "  1037,\n",
       "  2991,\n",
       "  1999,\n",
       "  4199,\n",
       "  1999,\n",
       "  1037,\n",
       "  6878,\n",
       "  13742,\n",
       "  102,\n",
       "  101,\n",
       "  2002,\n",
       "  2001,\n",
       "  2579,\n",
       "  2000,\n",
       "  1037,\n",
       "  2966,\n",
       "  4322,\n",
       "  1999,\n",
       "  1996,\n",
       "  3190,\n",
       "  2181,\n",
       "  1010,\n",
       "  2485,\n",
       "  2000,\n",
       "  2010,\n",
       "  2155,\n",
       "  2188,\n",
       "  1999,\n",
       "  8904,\n",
       "  3449,\n",
       "  9644,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  2002,\n",
       "  2351,\n",
       "  2006,\n",
       "  4465,\n",
       "  2012,\n",
       "  7855,\n",
       "  3986,\n",
       "  2902,\n",
       "  1011,\n",
       "  2966,\n",
       "  19684,\n",
       "  1005,\n",
       "  2015,\n",
       "  2436,\n",
       "  14056,\n",
       "  3581,\n",
       "  18454,\n",
       "  6199,\n",
       "  2319,\n",
       "  2758,\n",
       "  1037,\n",
       "  3426,\n",
       "  1997,\n",
       "  2331,\n",
       "  24185,\n",
       "  1050,\n",
       "  29618,\n",
       "  2102,\n",
       "  2022,\n",
       "  2207,\n",
       "  2127,\n",
       "  6928,\n",
       "  2012,\n",
       "  1996,\n",
       "  5700,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  3988,\n",
       "  2610,\n",
       "  4311,\n",
       "  5393,\n",
       "  1996,\n",
       "  2991,\n",
       "  2001,\n",
       "  2019,\n",
       "  4926,\n",
       "  2021,\n",
       "  4614,\n",
       "  2024,\n",
       "  11538,\n",
       "  1996,\n",
       "  6061,\n",
       "  2008,\n",
       "  9587,\n",
       "  29076,\n",
       "  2001,\n",
       "  20114,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  2006,\n",
       "  4465,\n",
       "  1010,\n",
       "  2010,\n",
       "  5542,\n",
       "  9460,\n",
       "  2626,\n",
       "  3784,\n",
       "  1024,\n",
       "  1036,\n",
       "  2023,\n",
       "  2851,\n",
       "  2026,\n",
       "  5542,\n",
       "  4080,\n",
       "  1005,\n",
       "  2015,\n",
       "  3969,\n",
       "  2001,\n",
       "  4196,\n",
       "  2039,\n",
       "  2000,\n",
       "  6014,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  3988,\n",
       "  2610,\n",
       "  4311,\n",
       "  5393,\n",
       "  1996,\n",
       "  2991,\n",
       "  2001,\n",
       "  2019,\n",
       "  4926,\n",
       "  2021,\n",
       "  4614,\n",
       "  2024,\n",
       "  11538,\n",
       "  1996,\n",
       "  6061,\n",
       "  2008,\n",
       "  9587,\n",
       "  29076,\n",
       "  2001,\n",
       "  20114,\n",
       "  102,\n",
       "  101,\n",
       "  1036,\n",
       "  2012,\n",
       "  1996,\n",
       "  2927,\n",
       "  1997,\n",
       "  2254,\n",
       "  2002,\n",
       "  2253,\n",
       "  2000,\n",
       "  4199,\n",
       "  2000,\n",
       "  2817,\n",
       "  7548,\n",
       "  1998,\n",
       "  2006,\n",
       "  1996,\n",
       "  2126,\n",
       "  2188,\n",
       "  2013,\n",
       "  1037,\n",
       "  2283,\n",
       "  2002,\n",
       "  2001,\n",
       "  23197,\n",
       "  4457,\n",
       "  1998,\n",
       "  6908,\n",
       "  2125,\n",
       "  1037,\n",
       "  2871,\n",
       "  6199,\n",
       "  2958,\n",
       "  1998,\n",
       "  2718,\n",
       "  1996,\n",
       "  5509,\n",
       "  2917,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  1036,\n",
       "  2002,\n",
       "  2001,\n",
       "  1999,\n",
       "  1037,\n",
       "  16571,\n",
       "  1998,\n",
       "  1999,\n",
       "  4187,\n",
       "  4650,\n",
       "  2005,\n",
       "  2706,\n",
       "  1012,\n",
       "  1005,\n",
       "  102,\n",
       "  101,\n",
       "  13723,\n",
       "  20073,\n",
       "  1010,\n",
       "  2040,\n",
       "  2056,\n",
       "  2016,\n",
       "  2003,\n",
       "  1037,\n",
       "  2485,\n",
       "  2155,\n",
       "  2767,\n",
       "  1010,\n",
       "  2409,\n",
       "  2026,\n",
       "  9282,\n",
       "  2166,\n",
       "  1010,\n",
       "  2008,\n",
       "  9587,\n",
       "  29076,\n",
       "  2018,\n",
       "  2069,\n",
       "  2042,\n",
       "  1999,\n",
       "  1996,\n",
       "  2406,\n",
       "  2005,\n",
       "  2416,\n",
       "  2847,\n",
       "  2043,\n",
       "  1996,\n",
       "  5043,\n",
       "  3047,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  2016,\n",
       "  2056,\n",
       "  2002,\n",
       "  2001,\n",
       "  2001,\n",
       "  2894,\n",
       "  2012,\n",
       "  1996,\n",
       "  2051,\n",
       "  1997,\n",
       "  1996,\n",
       "  6884,\n",
       "  6101,\n",
       "  1998,\n",
       "  3167,\n",
       "  5167,\n",
       "  2020,\n",
       "  7376,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  2016,\n",
       "  2794,\n",
       "  2008,\n",
       "  2002,\n",
       "  2001,\n",
       "  1999,\n",
       "  1037,\n",
       "  2512,\n",
       "  29624,\n",
       "  7583,\n",
       "  15004,\n",
       "  10572,\n",
       "  16571,\n",
       "  1010,\n",
       "  2383,\n",
       "  4265,\n",
       "  3809,\n",
       "  8985,\n",
       "  1998,\n",
       "  4722,\n",
       "  9524,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  9587,\n",
       "  29076,\n",
       "  2001,\n",
       "  1037,\n",
       "  2353,\n",
       "  29624,\n",
       "  29100,\n",
       "  5446,\n",
       "  2350,\n",
       "  2013,\n",
       "  8904,\n",
       "  3449,\n",
       "  9644,\n",
       "  1010,\n",
       "  5665,\n",
       "  29625,\n",
       "  1010,\n",
       "  2040,\n",
       "  2001,\n",
       "  8019,\n",
       "  1999,\n",
       "  1037,\n",
       "  13609,\n",
       "  29624,\n",
       "  10052,\n",
       "  2565,\n",
       "  2012,\n",
       "  2198,\n",
       "  9298,\n",
       "  4140,\n",
       "  2118,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  9587,\n",
       "  29076,\n",
       "  6272,\n",
       "  2000,\n",
       "  1996,\n",
       "  2082,\n",
       "  1005,\n",
       "  2015,\n",
       "  3127,\n",
       "  1997,\n",
       "  1996,\n",
       "  13201,\n",
       "  16371,\n",
       "  13577,\n",
       "  1010,\n",
       "  4311,\n",
       "  1996,\n",
       "  3190,\n",
       "  10969,\n",
       "  2040,\n",
       "  6866,\n",
       "  1037,\n",
       "  3696,\n",
       "  2648,\n",
       "  1037,\n",
       "  2311,\n",
       "  3752,\n",
       "  1036,\n",
       "  11839,\n",
       "  2005,\n",
       "  9587,\n",
       "  29076,\n",
       "  1012,\n",
       "  1005,\n",
       "  102,\n",
       "  101,\n",
       "  1996,\n",
       "  13577,\n",
       "  1005,\n",
       "  2015,\n",
       "  5947,\n",
       "  3127,\n",
       "  2623,\n",
       "  4465,\n",
       "  5027,\n",
       "  3081,\n",
       "  10474,\n",
       "  2008,\n",
       "  1037,\n",
       "  3986,\n",
       "  2326,\n",
       "  2097,\n",
       "  2022,\n",
       "  2218,\n",
       "  2006,\n",
       "  3721,\n",
       "  2000,\n",
       "  3342,\n",
       "  9587,\n",
       "  29076,\n",
       "  1012,\n",
       "  102],\n",
       " 'tgt': [1,\n",
       "  4080,\n",
       "  9587,\n",
       "  29076,\n",
       "  1010,\n",
       "  2322,\n",
       "  1010,\n",
       "  2013,\n",
       "  8904,\n",
       "  3449,\n",
       "  9644,\n",
       "  1010,\n",
       "  4307,\n",
       "  1010,\n",
       "  2018,\n",
       "  2069,\n",
       "  2074,\n",
       "  3369,\n",
       "  2005,\n",
       "  1037,\n",
       "  13609,\n",
       "  2565,\n",
       "  2043,\n",
       "  1996,\n",
       "  5043,\n",
       "  3047,\n",
       "  1999,\n",
       "  2254,\n",
       "  3,\n",
       "  2002,\n",
       "  2001,\n",
       "  10583,\n",
       "  2067,\n",
       "  2000,\n",
       "  3190,\n",
       "  3081,\n",
       "  2250,\n",
       "  2006,\n",
       "  2233,\n",
       "  2322,\n",
       "  2021,\n",
       "  2002,\n",
       "  2351,\n",
       "  2006,\n",
       "  4465,\n",
       "  3,\n",
       "  3988,\n",
       "  2610,\n",
       "  4311,\n",
       "  5393,\n",
       "  1996,\n",
       "  2991,\n",
       "  2001,\n",
       "  2019,\n",
       "  4926,\n",
       "  2021,\n",
       "  4614,\n",
       "  2024,\n",
       "  11538,\n",
       "  1996,\n",
       "  6061,\n",
       "  2008,\n",
       "  9587,\n",
       "  29076,\n",
       "  2001,\n",
       "  20114,\n",
       "  3,\n",
       "  2010,\n",
       "  5542,\n",
       "  4447,\n",
       "  2002,\n",
       "  2001,\n",
       "  4457,\n",
       "  1998,\n",
       "  6908,\n",
       "  2871,\n",
       "  6199,\n",
       "  2013,\n",
       "  1037,\n",
       "  2958,\n",
       "  2],\n",
       " 'src_sent_labels': [0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'segs': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'clss': [0,\n",
       "  25,\n",
       "  57,\n",
       "  78,\n",
       "  112,\n",
       "  136,\n",
       "  174,\n",
       "  197,\n",
       "  223,\n",
       "  245,\n",
       "  285,\n",
       "  301,\n",
       "  337,\n",
       "  358,\n",
       "  382,\n",
       "  416,\n",
       "  452],\n",
       " 'src_txt': ['a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .',\n",
       "  'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .',\n",
       "  'he was flown back to chicago via air ambulance on march 20 , but he died on sunday .',\n",
       "  'andrew mogni , 20 , from glen ellyn , illinois , a university of iowa student has died nearly three months after a fall in rome in a suspected robbery',\n",
       "  'he was taken to a medical facility in the chicago area , close to his family home in glen ellyn .',\n",
       "  \"he died on sunday at northwestern memorial hospital - medical examiner 's office spokesman frank shuftan says a cause of death wo n't be released until monday at the earliest .\",\n",
       "  'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed .',\n",
       "  \"on sunday , his cousin abby wrote online : ` this morning my cousin andrew 's soul was lifted up to heaven .\",\n",
       "  'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed',\n",
       "  '` at the beginning of january he went to rome to study aboard and on the way home from a party he was brutally attacked and thrown off a 40ft bridge and hit the concrete below .',\n",
       "  \"` he was in a coma and in critical condition for months . '\",\n",
       "  'paula barnett , who said she is a close family friend , told my suburban life , that mogni had only been in the country for six hours when the incident happened .',\n",
       "  'she said he was was alone at the time of the alleged assault and personal items were stolen .',\n",
       "  'she added that he was in a non-medically induced coma , having suffered serious infection and internal bleeding .',\n",
       "  'mogni was a third-year finance major from glen ellyn , ill. , who was participating in a semester-long program at john cabot university .',\n",
       "  \"mogni belonged to the school 's chapter of the sigma nu fraternity , reports the chicago tribune who posted a sign outside a building reading ` pray for mogni . '\",\n",
       "  \"the fraternity 's iowa chapter announced sunday afternoon via twitter that a memorial service will be held on campus to remember mogni .\"],\n",
       " 'tgt_txt': 'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: './data/temp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-e64cbd30c0f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNamespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mget_candidates_mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-ef9a143e643a>\u001b[0m in \u001b[0;36mget_candidates_mp\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total {} documents'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0mprocessed_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'processed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs109b_FP/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './data/temp'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --tokenizer TOKENIZER --data_path DATA_PATH\n",
      "                             --index_path INDEX_PATH --write_path WRITE_PATH\n",
      "ipykernel_launcher.py: error: the following arguments are required: --tokenizer, --data_path, --index_path, --write_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "MAX_LEN = 512\n",
    "\n",
    "_ROUGE_PATH = './ROUGE-1.5.5'\n",
    "temp_path = './data/temp' # path to store some temporary files\n",
    "\n",
    "original_data, sent_ids = [], []\n",
    "\n",
    "def load_jsonl(data_path):\n",
    "    data = []\n",
    "    with open(data_path) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def get_rouge(path, dec):\n",
    "    log.get_global_console_logger().setLevel(logging.WARNING)\n",
    "    dec_pattern = '(\\d+).dec'\n",
    "    ref_pattern = '#ID#.ref'\n",
    "    dec_dir = join(path, 'decode')\n",
    "    ref_dir = join(path, 'reference')\n",
    "\n",
    "    with open(join(dec_dir, '0.dec'), 'w') as f:\n",
    "        for sentence in dec:\n",
    "            print(sentence, file=f)\n",
    "\n",
    "    cmd = '-c 95 -r 1000 -n 2 -m'\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        Rouge155.convert_summaries_to_rouge_format(\n",
    "            dec_dir, join(tmp_dir, 'dec'))\n",
    "        Rouge155.convert_summaries_to_rouge_format(\n",
    "            ref_dir, join(tmp_dir, 'ref'))\n",
    "        Rouge155.write_config_static(\n",
    "            join(tmp_dir, 'dec'), dec_pattern,\n",
    "            join(tmp_dir, 'ref'), ref_pattern,\n",
    "            join(tmp_dir, 'settings.xml'), system_id=1\n",
    "        )\n",
    "        cmd = (join(_ROUGE_PATH, 'ROUGE-1.5.5.pl')\n",
    "            + ' -e {} '.format(join(_ROUGE_PATH, 'data'))\n",
    "            + cmd\n",
    "            + ' -a {}'.format(join(tmp_dir, 'settings.xml')))\n",
    "        output = sp.check_output(cmd.split(' '), universal_newlines=True)\n",
    "\n",
    "        line = output.split('\\n')\n",
    "        rouge1 = float(line[3].split(' ')[3])\n",
    "        rouge2 = float(line[7].split(' ')[3])\n",
    "        rougel = float(line[11].split(' ')[3])\n",
    "    return (rouge1 + rouge2 + rougel) / 3\n",
    "\n",
    "@curry\n",
    "def get_candidates(tokenizer, cls, sep_id, idx):\n",
    "\n",
    "    idx_path = join(temp_path, str(idx))\n",
    "    \n",
    "    # create some temporary files to calculate ROUGE\n",
    "    sp.call('mkdir ' + idx_path, shell=True)\n",
    "    sp.call('mkdir ' + join(idx_path, 'decode'), shell=True)\n",
    "    sp.call('mkdir ' + join(idx_path, 'reference'), shell=True)\n",
    "    \n",
    "    # load data\n",
    "    data = {}\n",
    "    data['text'] = original_data[idx]['text']\n",
    "    data['summary'] = original_data[idx]['summary']\n",
    "    \n",
    "    # write reference summary to temporary files\n",
    "    ref_dir = join(idx_path, 'reference')\n",
    "    with open(join(ref_dir, '0.ref'), 'w') as f:\n",
    "        for sentence in data['summary']:\n",
    "            print(sentence, file=f)\n",
    "\n",
    "    # get candidate summaries\n",
    "    # here is for CNN/DM: truncate each document into the 5 most important sentences (using BertExt), \n",
    "    # then select any 2 or 3 sentences to form a candidate summary, so there are C(5,2)+C(5,3)=20 candidate summaries.\n",
    "    # if you want to process other datasets, you may need to adjust these numbers according to specific situation.\n",
    "    sent_id = sent_ids[idx]['sent_id'][:5]\n",
    "    indices = list(combinations(sent_id, 2))\n",
    "    indices += list(combinations(sent_id, 3))\n",
    "    if len(sent_id) < 2:\n",
    "        indices = [sent_id]\n",
    "    \n",
    "    # get ROUGE score for each candidate summary and sort them in descending order\n",
    "    score = []\n",
    "    for i in indices:\n",
    "        i = list(i)\n",
    "        i.sort()\n",
    "        # write dec\n",
    "        dec = []\n",
    "        for j in i:\n",
    "            sent = data['text'][j]\n",
    "            dec.append(sent)\n",
    "        score.append((i, get_rouge(idx_path, dec)))\n",
    "    score.sort(key=lambda x : x[1], reverse=True)\n",
    "    \n",
    "    # write candidate indices and score\n",
    "    data['ext_idx'] = sent_id\n",
    "    data['indices'] = []\n",
    "    data['score'] = []\n",
    "    for i, R in score:\n",
    "        data['indices'].append(list(map(int, i)))\n",
    "        data['score'].append(R)\n",
    "\n",
    "    # tokenize and get candidate_id\n",
    "    candidate_summary = []\n",
    "    for i in data['indices']:\n",
    "        cur_summary = [cls]\n",
    "        for j in i:\n",
    "            cur_summary += data['text'][j].split()\n",
    "        cur_summary = cur_summary[:MAX_LEN]\n",
    "        cur_summary = ' '.join(cur_summary)\n",
    "        candidate_summary.append(cur_summary)\n",
    "    \n",
    "    data['candidate_id'] = []\n",
    "    for summary in candidate_summary:\n",
    "        token_ids = tokenizer.encode(summary, add_special_tokens=False)[:(MAX_LEN - 1)]\n",
    "        token_ids += sep_id\n",
    "        data['candidate_id'].append(token_ids)\n",
    "    \n",
    "    # tokenize and get text_id\n",
    "    text = [cls]\n",
    "    for sent in data['text']:\n",
    "        text += sent.split()\n",
    "    text = text[:MAX_LEN]\n",
    "    text = ' '.join(text)\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)[:(MAX_LEN - 1)]\n",
    "    token_ids += sep_id\n",
    "    data['text_id'] = token_ids\n",
    "    \n",
    "    # tokenize and get summary_id\n",
    "    summary = [cls]\n",
    "    for sent in data['summary']:\n",
    "        summary += sent.split()\n",
    "    summary = summary[:MAX_LEN]\n",
    "    summary = ' '.join(summary)\n",
    "    token_ids = tokenizer.encode(summary, add_special_tokens=False)[:(MAX_LEN - 1)]\n",
    "    token_ids += sep_id\n",
    "    data['summary_id'] = token_ids\n",
    "    \n",
    "    # write processed data to temporary file\n",
    "    processed_path = join(temp_path, 'processed')\n",
    "    with open(join(processed_path, '{}.json'.format(idx)), 'w') as f:\n",
    "        json.dump(data, f, indent=4) \n",
    "    \n",
    "    sp.call('rm -r ' + idx_path, shell=True)\n",
    "\n",
    "def get_candidates_mp(args):\n",
    "    \n",
    "    # choose tokenizer\n",
    "    if args.tokenizer == 'bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        cls, sep = '[CLS]', '[SEP]'\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        cls, sep = '<s>', '</s>'\n",
    "    sep_id = tokenizer.encode(sep, add_special_tokens=False)\n",
    "\n",
    "    # load original data and indices\n",
    "    global original_data, sent_ids\n",
    "    original_data = load_jsonl(args.data_path)\n",
    "    original_data = original_data[0]\n",
    "    sent_ids = load_jsonl(args.index_path)\n",
    "    n_files = len(original_data)\n",
    "    assert len(sent_ids) == len(original_data)\n",
    "    print('total {} documents'.format(n_files))\n",
    "    os.makedirs(temp_path)\n",
    "    processed_path = join(temp_path, 'processed')\n",
    "    os.makedirs(processed_path)\n",
    "\n",
    "    # use multi-processing to get candidate summaries\n",
    "    start = time()\n",
    "    print('start getting candidates with multi-processing !!!')\n",
    "    \n",
    "    with mp.Pool() as pool:\n",
    "        list(pool.imap_unordered(get_candidates(tokenizer, cls, sep_id), range(n_files), chunksize=64))\n",
    "    \n",
    "    print('finished in {}'.format(timedelta(seconds=time()-start)))\n",
    "    \n",
    "    # write processed data\n",
    "    print('start writing {} files'.format(n_files))\n",
    "    for i in range(n_files):\n",
    "        with open(join(processed_path, '{}.json'.format(i))) as f:\n",
    "            data = json.loads(f.read())\n",
    "        with open(args.write_path, 'a') as f:\n",
    "            print(json.dumps(data), file=f)\n",
    "    \n",
    "    os.system('rm -r {}'.format(temp_path))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Process truncated documents to obtain candidate summaries'\n",
    "    )\n",
    "    parser.add_argument('--tokenizer', type=str, required=True,\n",
    "        help='BERT/RoBERTa')\n",
    "    parser.add_argument('--data_path', type=str, required=True,\n",
    "        help='path to the original dataset, the original dataset should contain text and summary')\n",
    "    parser.add_argument('--index_path', type=str, required=True,\n",
    "        help='indices of the remaining sentences of the truncated document')\n",
    "    parser.add_argument('--write_path', type=str, required=True,\n",
    "        help='path to store the processed dataset')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    assert args.tokenizer in ['bert', 'roberta']\n",
    "    assert exists(args.data_path)\n",
    "    assert exists(args.index_path)\n",
    "\n",
    "    get_candidates_mp(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 100 documents\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: './data/temp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-e64cbd30c0f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNamespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mget_candidates_mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-ef9a143e643a>\u001b[0m in \u001b[0;36mget_candidates_mp\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total {} documents'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0mprocessed_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'processed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs109b_FP/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './data/temp'"
     ]
    }
   ],
   "source": [
    "args_dict = {'tokenizer':'BERT',\n",
    "             'data_path':'./data/bert_data_jon/match_summ_sample.json',\n",
    "             'index_path':'./data/bert_data_jon/sentence_id.json',\n",
    "             'write_path':'./data/bert_data_jon/processed_data.jsonl'\n",
    "            }\n",
    "args = Namespace(**args_dict)\n",
    "\n",
    "get_candidates_mp(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-76c651f4872a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-76c651f4872a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python get_candidate.py --tokenizer='bert' --data_path='/presumm_data/tokenized_text/opinions/match_summ_sample.json' --index_path='/presumm_data/tokenized_text/headnotes/sentence_id.json' --write_path='/presumm_data/your_processed_data.jsonl'\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "class MatchSum(nn.Module):\n",
    "    \n",
    "    def __init__(self, candidate_num, encoder, hidden_size=768):\n",
    "        super(MatchSum, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.candidate_num  = candidate_num\n",
    "        \n",
    "        if encoder == 'bert':\n",
    "            self.encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "        else:\n",
    "            self.encoder = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "    def forward(self, text_id, candidate_id, summary_id):\n",
    "        \n",
    "        batch_size = text_id.size(0)\n",
    "        \n",
    "        # get document embedding\n",
    "        input_mask = ~(text_id == 0)\n",
    "        out = self.encoder(text_id, attention_mask=input_mask)[0] # last layer\n",
    "        doc_emb = out[:, 0, :]\n",
    "        assert doc_emb.size() == (batch_size, self.hidden_size) # [batch_size, hidden_size]\n",
    "        \n",
    "        # get summary embedding\n",
    "        input_mask = ~(summary_id == 0)\n",
    "        out = self.encoder(summary_id, attention_mask=input_mask)[0] # last layer\n",
    "        summary_emb = out[:, 0, :]\n",
    "        assert summary_emb.size() == (batch_size, self.hidden_size) # [batch_size, hidden_size]\n",
    "\n",
    "        # get summary score\n",
    "        summary_score = torch.cosine_similarity(summary_emb, doc_emb, dim=-1)\n",
    "\n",
    "        # get candidate embedding\n",
    "        candidate_num = candidate_id.size(1)\n",
    "        candidate_id = candidate_id.view(-1, candidate_id.size(-1))\n",
    "        input_mask = ~(candidate_id == 0)\n",
    "        out = self.encoder(candidate_id, attention_mask=input_mask)[0]\n",
    "        candidate_emb = out[:, 0, :].view(batch_size, candidate_num, self.hidden_size)  # [batch_size, candidate_num, hidden_size]\n",
    "        assert candidate_emb.size() == (batch_size, candidate_num, self.hidden_size)\n",
    "        \n",
    "        # get candidate score\n",
    "        doc_emb = doc_emb.unsqueeze(1).expand_as(candidate_emb)\n",
    "        score = torch.cosine_similarity(candidate_emb, doc_emb, dim=-1) # [batch_size, candidate_num]\n",
    "        assert score.size() == (batch_size, candidate_num)\n",
    "\n",
    "        return {'score': score, 'summary_score': summary_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae1f684e4094273b72853b11b3b0cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59727e2e50134b7198cfa879da4342df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = MatchSum(5,'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of MatchSum(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('MatchSum_cnndm_model/MatchSum_cnndm_bert.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
